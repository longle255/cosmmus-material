%!TEX root =  main.tex

\section{RamCast: RDMA-based Atomic Multicast}
\label{sec:rdma-atomic-multicast}

In this section, we explain the general design and architecture of \libname (\S\ref{sec:overview}), then detail \libname's atomic multicast algorithm in the normal case without failures (\S\ref{sec:normalcase}), and in the presence of failures (\S\ref{sec:failurecase}).

\subsection{Building blocks}
\label{sec:overview}

\libname leverages two ideas, Skeen's atomic multicast algorithm \cite{BJ87b} and Protected Memory Paxos \cite{Aguilera2019}.
Skeen's algorithm orders messages multicast to multiple processes consistently but it does not tolerate failures.
Protected Memory Paxos takes advantage of RDMA permissions to improve the efficiency of Paxos \cite{L98}. 
Like Paxos, it implements atomic broadcast, that is, it assumes a single group of processes.
Combining these ideas coherently and efficiently is not trivial, as we discuss in this section,
after briefly presenting Skeen's algorithm and Protected Memory Paxos.

%\subsubsection{Skeen's atomic multicast}

\subsubsection{Skeen's atomic multicast}

In Skeen's algorithm, each process and assigns unique timestamps to multicast messages based on a logical clock~\cite{Lam78}.
The correctness of the algorithm stems from two basic properties:
(i)~processes in the destination of a multicast message first assign tentative timestamps to the message and eventually agree on the message's final timestamp; and
(ii)~processes deliver messages according to their final timestamp.
These properties are implemented as follow.

\begin{itemize}
\item[(i)] To multicast a message $m$ to a set of processes, $p$ sends $m$ to the destinations.
Upon receiving $m$, each destination updates its logical clock, assigns a tentative timestamp to $m$, stores $m$ and its timestamp in a buffer, and sends $m$'s timestamp to all destinations.
Upon receiving timestamps from all destinations in $m.dst$, a process computes $m$'s final timestamp as the maximum among all received tentative timestamps for $m$.
\item[(ii)]Messages are delivered respecting the order of their final timestamp.
A process $p$ delivers $m$ when it can ascertain that $m$'s final timestamp is smaller than the final timestamp of any messages $p$ will deliver after $m$ (intuitively, this holds because logical clocks are monotonically increasing).
\end{itemize}

\subsubsection{Paxos and Protected Memory Paxos}

In Paxos, in order for the leader to order a message $m$, it assigns $m$ to a consensus instance and proposes that $m$ be accepted in the proposed instance.
In the normal case, where there is a single leader, the followers accept the proposed message and reply to leader.
Protected Memory Paxos optimizes this procedure by granting to the leader exclusive write permission to the memory of the followers.
If another process becomes leader, then it takes the exclusive permission.
The idea is that the leader writes the proposed message in the memory of the followers.
If the leader succeeds to write the message in the memory of a quorum of followers, then it is because no other leader took over.
This happens because if a leader takes over, then it revokes the permission of the previous leader.

Just like Paxos, to ensure that the new leader makes decisions that are consistent with the decisions of the previous leader, each leader associates a \emph{round} to its proposed message.
%A round is a tuple $\langle rnd, pid \rangle$, where $rnd$ is a scalar and $pid$ is the process unique identifier, used to make rounds unique across the system.
Rounds are unique across the system.
%The very first leader, say process $p$, uses round $\langle 0, p \rangle$.
When process $q$ becomes leader, upon suspecting $p$'s failure, $q$ must pick a round that is bigger than $p$'s round.
Process $q$ then proceeds in two steps.
First, $q$ needs to acquire permission from a quorum of processes, which it does by contacting all processes and providing its chosen round.
Processes grant write permission to $q$ if the provided round is bigger than the round of the process that current holds write permission.
Second, $q$ must check other processes have already accepted any values.
If so, $q$ must propose the value that has been accepted in the largest round; otherwise, $q$ can propose any values.

\subsection{Overall design and architecture}

Figure~\ref{fig:arch} depicts the various components and memory layout of \libname.
Processes within each group coordinate using the leader-follower model, like in Paxos and Protected Memory Paxos.
Each server process has a fixed-size circular buffer per client, analogously to other RDMA-based systems \cite{FaRM, Mu, DARE, APUS}.
A circular buffer in \libname is divided into two parts, a message buffer $M$, and a timestamp buffer $T$.
Message buffer $M$ is a shared memory region that can be read and written by any other processes, including the client process the buffer is associated with.
Timestamp buffer $T$ is protected and can only be written by the leader of each group; the buffer can be read by any processes.
Each entry in $M$, with a multicast message $m$, has a corresponding entry in $T$, with $m$'s timestamp.

%Each server process organizes its memory in regions: a shared
%memory region and a protected memory region. All processes have remote access
%(read/write) to the shared memory space. 
%Only one leader of each group
%has remote-write permission to a given node's log at any point in time
%during the protocol.

Clients keep a copy of the remote head and tail pointer of their buffer at each server. 
A client increases the remote tail after writing to the shared memory. 
The server process updates the head pointer on the client buffer after handling the message by piggybacking the new value in the response.
Each process $p$ periodically polls the memory cell at the head position of each
connected QPs to detect new messages.
%\libname maintain quorums of ``candidate'' processes of each group to be the leader of that group.




\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/architecture2}
  \caption{\libname's memory layout and execution steps: in step 1, the client writes a multicast message in the memory of all destination processes; in step 2 the leader of each destination group proposes and writes a timestamp for the message in the memory of its followers and other leaders; in step 3 the leaders propagate the timestamp written by other leaders, and the followers acknowledge the proposed timestamps. The message is delivered after step 3.}
%Processes have shared and exclusive memory that can be accessed by RDMA primitives. Exclusive memory access needs permission. A client proposes new message by writing to the shared memory of all processes. Leader processes writes their proposed timestamps to exclusive memory. Follower processes write their acknowledgement to the shared memory}
  \label{fig:arch}
\end{figure}


\libname consists of the following main components:
\begin{itemize}
  \item \emph{Memory management.} Processes separate their memory into two regions: a shared buffer and a protected buffer (detailed in \S\ref{sec:ds-structs}). While all processes have access to read and write the shared buffer of a process, only the leader of each group has permission to write the protected buffer of a process. Each process grants and revokes write permission to its protected buffer, to ensure only one leader of each group has write access at a time.
  \item \emph{RDMA communication.} This component provides functions to read and write remote memory, used by the normal operation component, and to send and receive messages, used by the failure handling module.
  \item \emph{Normal execution.} The normal protocol execution (detailed in \S\ref{sec:normalcase}) is invoked when there is a sole leader per group with support of at least a majority of processes in its group. A leader is responsible for proposing the group's timestamp for the new message, and propagating timestamps from other groups to followers of its local group.
  \item \emph{Failure handling.} Upon detecting the failure of a leader, processes in a group elect a new leader. The new leader must ensure that its execution is consistent with the execution of the previous leader (detailed in \S\ref{sec:failurecase}).
  \item \emph{Leader election.} \libname requires processes to detect a slow or crashed leader, and elect a new leader. Leader election is not assumed to be perfect: the protocol ensures safety despite multiple leaders in a group. To ensure progress, though, eventually every group should elect a single operational and stable leader \cite{Aguilera2019,L98}.
Stable leader election can be implemented in the partially synchronous model \cite{Aguilera2001}.
\end{itemize}

\subsection{Data structures}
\label{sec:ds-structs}

Algorithm~\ref{alg:data_struct} presents the data structures used by processes in \libname.
Every server process has a shared circular buffer $M[c,-]$ per client, where in each entry the client writes a multicast message $msg$, the groups $dst$ the message is addressed to, and a vector $ptr[p]$ with an entry per process $p$ with the address of the entry in $p$'s memory. The address vector is used by other processes to access the entry in $p$'s memory.
Servers compute the message's timestamp $tmp$, based on the timestamps proposed by the leader of each destination group, and the acknowledgement $ack[p]$ from each follower $p$ in the leader's group.
A timestamp is a tuple $(cnt,pid)$, where $cnt$ is a scalar and $pid$ is a process identifier, used to break ties.
A multicast message state $stat$ can be null ($\perp$), pending a final timestamp (\mcast), assigned a final timestamp (\ordered) or delivered (\done).

To compute a message's timestamp, each server process has a protected circular buffer $T$, where the $i$-th entry $T[c,i]$ matches the corresponding entry $M[c,i]$ in the shared circular buffer $M$ associated with $c$.
The entry contains a timestamp $tmp[g]$ per group $g$ in the message's destination, proposed by the current leader in $g$, and the round $rnd[g]$ when the leader proposed the message.\footnote{Timestamps and rounds are tuples $\langle cnt,pid \rangle$, where $cnt$ is a scalar and $pid$ is a process identifier. It follows that $\langle cnt,pid \rangle > \langle cnt',pid' \rangle$ iff $cnt > cnt'$, or $cnt = cnt'$ and $pid > pid'$.}

Process $p$'s local state includes a logical timestamp counter, $p$'s current round, vector $Leader[g]$ with $p$'s view on the current leader of group $g$, and the last round $Round[g]$ that $p$ accepted for group $g$.

\input{ds-structs}


%\begin{figure}[ht!]
%  \centering
%  \includegraphics[width=1\linewidth]{figures/memory}
%  \caption{Memory layout of \libname. Each process has shared and exclusive memory
%  space. All other processes can access shared memory. Only leader can access
%  exclusive memory }
%  \label{fig:normal_operation_time}
%\end{figure}







\subsection{Normal execution}
\label{sec:normalcase}


\input{algo-amcast}
% \input{algo-normal-case}

Algorithm \ref{alg:normal_case} presents \libname's normal execution, when the leaders of the groups a message is addressed to are operational and stable.
To multicast a message $m$ (Task 1 in Algorithm \ref{alg:normal_case}), a client $c$ first calculates the next available entry in its circular buffer stored in the memory of every process addressed by $m$, and then invokes the $Relay$ procedure (Algorithm~\ref{alg:data_struct}), which copies the message, its destination, and the address vector for $m$ in the circular buffer of every process addressed by $m$.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/execution}
  \caption{Normal case execution of \libname. To avoid cluttering the figure, we show the execution of only one follower per group.}
  \label{fig:normal_operation_time}
\end{figure}

Once leader process $L$ in group $g$ reads $m$ in its message buffer (Task 2), it computes a
group-wise timestamp for $m$ and writes the proposed timestamp to the corresponding entry
in the protected timestamp buffer of (1) all follower processes in the leader's group $g$, and (2) all leader processes 
of other groups in the destination of $m$.
If a remote write is denied, then $L$ ends the task since another process in $g$ became leader.

When a follower detects that a multicast message has been assigned a timestamp by the group leader (Task 3), it updates its clock, to ensure that timestamps from a group are monotonically increasing (necessary in case the process becomes leader) and acknowledges that it has read this timestamp to every process in the destination of the message.
The follower detects the leader proposed timestamp by checking whether the round of the timestamp matches the round associated with the group leader. 
This is enough to ensure that the timestamp has been also updated by the leader, since the round is updated later in Task 2 (we discuss in Section~\ref{sec:implementation} how this is ensured in RDMA).

\textbf{[Long will check this]} When a process reads a timestamp written by a remote leader (Task 4), it updates its local clock with the timestamp, and writes the timestamp in the memory of each follower.
This task is executed by the leader of a group only, since only the leader is updated with remote timestamps (see Task 2).
The reason why only the leader of a group is updated with timestamps proposed by remote leaders in the message destination, is to ensure that any timestamps assigned by the leader are consistent with any other timestamps assigned by the group, similarly to other leader-follower-based protocols \cite{gotsman2019white, Junqueira2011}.
%Once each involved process $P$ belongs to $m.dst$ \lread message $m$ in its
%local buffer, $P$ mark the message as pending, and starts polling from it
%exclusive memory for timestamps of $m$ (Task 4 algorithm~\ref{alg:normal_case}).

Upon \lread a timestamp $ts$ of message $m$ from leader of local group, a
process $P$ writes its acknowledgement $ack$ for that timestamp to the shared
memory of all other involved processes, and start polling from its memory for
acknowledgements from other processes. (Task 4 algorithm~\ref{alg:normal_case}).
Once $P$ receives timestamps of all involved groups and acknowledgements from
the majority of processes of each group, it can order the message $m$ and starts
deliver it.

After delivering a message, a process will keep that message in its local buffer.
Upon receiving acknowledgements from all the processes in the candidate
quorum, the message will be removed from the buffer.

\subsection{Handling failures}
\label{sec:failurecase}

%\input{algo-leader-election.tex}

When a process becomes a leader, it needs to perform
several steps to ensure it is the only leader of its groups. Those steps are:
(1) increasing ballot number, in which that process becomes leader. (2) sending
1A message to all processes with new ballot number, and wait for the response of
majority (3) ensure the logs of all processes are up-to-date

Firstly, the new leader must choose a new ballot number that is higher than all
ballot numbers before for its group. A process only reply to the message or
timestamp that has a ballot number higher than its current value. The new ballot
number will be included in each timestamp $ts$ of this leader later.

The new leader needs to get the access to the exclusive memory space on (1) the
followers of its local group, and (2) the leaders of other groups. In order to
act as a leader, a node must get the write permission from majority of other
processes in its group. In addition, to tolerate failure of the leaders of other
groups, the new leader also need to get the write permission from majority of
processes of other groups. The new leader request the access by issuing a RDMA
Send (\lle{this could be a \rwrite}) 1A message that includes a new ballot
number all involved processes. Upon receiving such message, a process first
checks if the proposed ballot number is higher than the current ballot number it
stored. In such case, the process revokes access of the old leader, and replies
to the new leader with an 1B message that also contains the status of all
message it is processing (i.e., messages that are pending and messages that has
been fulfilled and are waiting to be delivered)

Once the new leader receives 1B reply from majority processes of each group, it
must ensure all remote logs are up-to-date with its own. If a message m is
fulfilled in some processes, the leader stores it in a settled list. If a
message is not fulfilled in any process, there are two cases: i)  the message
does not have the timestamp or enough acknowledgements from quorum of processes
of the group that has failed leader on any process ii) it does have such data on
some processes.\lle{sloppy sentence}

\input{algo-request-permission}

% If the
% sequence number of leader is smaller than follower, the leader should know that
% for missing instances, it can read the agreed timestamps from followers.

% the
% sequence number of the most recent delivered message to all involved processes.
% When a process receives this message, it checks if the ballot number is higher
% than its current ballot number, then revokes access of the old leader, and send a
% reply to the new leader with its current sequence number.



% Finally, once the new leader receives reply from majority processes, it must
% ensure all remote logs are up-to-date with its own. If the sequence number of
% leader is smaller than follower, the leader should know that for missing
% instances, it can read the agreed timestamps from followers.

% With the pending instances (the instances in the pending list without
% timestamps, or has not received majority of acks), leader can just rewrite new
% timestamp value with its new ballot number. In this case leader issues a RDMA
% WRITE-WITH-IMM which triggers a completion event at the receivers side.


% not necessary
% if the sequence number of leader is bigger than follower, the follower can do a
% \rread of missing timestamps from leader's memory.
