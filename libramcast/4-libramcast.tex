%!TEX root =  main.tex

\section{RamCast: RDMA-based Atomic Multicast}
\label{sec:rdma-atomic-multicast}

In this section, we recall the basic abstractions that inspired \libname (\S\ref{sec:bblocks}) and present its design and algorithms. 
We start with an overview of the protocol (\S\ref{sec:overview}), then detail \libname's data structures (\S\ref{sec:ds-structs}) and algorithms in the normal case without failures (\S\ref{sec:normalcase}) and in the presence of failures (\S\ref{sec:failurecase}).

\subsection{Building blocks}
\label{sec:bblocks}

\libname leverages two ideas, Skeen's atomic multicast algorithm \cite{BJ87b} and Protected Memory Paxos \cite{Aguilera2019}.
Skeen's algorithm orders messages multicast to multiple processes consistently but it does not tolerate failures.
Protected Memory Paxos takes advantage of RDMA permissions to improve the efficiency of Paxos \cite{L98}. 
Like Paxos, it implements atomic broadcast, that is, it assumes a single group of processes.
Combining these ideas coherently and efficiently is not trivial, as we discuss in this section,
after briefly presenting Skeen's algorithm and Protected Memory Paxos.

%\subsubsection{Skeen's atomic multicast}

\subsubsection{Skeen's atomic multicast}

In Skeen's algorithm, each process assigns unique timestamps to multicast messages based on a logical clock~\cite{Lam78}.
The correctness of the algorithm stems from two basic properties:
(i)~processes in the destination of a multicast message first assign tentative timestamps to the message and eventually agree on the message's final timestamp; and
(ii)~processes deliver messages according to their final timestamp.
These properties are implemented as follow.

\begin{itemize}
\item[(i)] To multicast a message $m$ to a set of processes, $p$ sends $m$ to the destinations.
Upon receiving $m$, each destination updates its logical clock, assigns a tentative timestamp to $m$, stores $m$ and its timestamp in a buffer, and sends $m$'s timestamp to all destinations.
Upon receiving timestamps from all destinations in $m.dst$, a process computes $m$'s final timestamp as the maximum among all received tentative timestamps for $m$.
\item[(ii)]Messages are delivered respecting the order of their final timestamp.
A process $p$ delivers $m$ when it can ascertain that $m$'s final timestamp is smaller than the final timestamp of any messages $p$ will deliver after $m$ (intuitively, this holds because logical clocks are monotonically increasing).
\end{itemize}

\subsubsection{Paxos and Protected Memory Paxos}

In Paxos, in order for the leader to order a message $m$, it assigns $m$ to a consensus instance and proposes that $m$ be accepted in the proposed instance.
In the normal case, where there is a single leader, the followers accept the proposed message and reply to leader.
Protected Memory Paxos optimizes this procedure by granting to the leader exclusive write permission to the memory of the followers.
If another process becomes leader, then it takes the exclusive permission.
The idea is that the leader writes the proposed message in the memory of the followers.
If the leader succeeds to write the message in the memory of a quorum of followers, then it is because no other leader took over.
This happens because if a leader takes over, then it revokes the permission of the previous leader.

Just like Paxos, to ensure that the new leader makes decisions that are consistent with the decisions of the previous leader, each leader associates a \emph{round} to its proposed message.
%A round is a tuple $\langle rnd, pid \rangle$, where $rnd$ is a scalar and $pid$ is the process unique identifier, used to make rounds unique across the system.
Rounds are unique across the system.
%The very first leader, say process $p$, uses round $\langle 0, p \rangle$.
When process $q$ becomes leader, upon suspecting $p$'s failure, $q$ must pick a round that is bigger than $p$'s round.
Process $q$ then proceeds in two steps.
First, $q$ needs to acquire permission from a quorum of processes, which it does by contacting all processes and providing its chosen round.
Processes grant write permission to $q$ if the provided round is bigger than the round of the process that currently holds the write permission.
Second, $q$ must check other processes have already accepted any values.
If so, $q$ must propose the value that has been accepted in the largest round; otherwise, $q$ can propose any values.

\subsection{Overall design and architecture}
\label{sec:overview}

Figure~\ref{fig:arch} depicts the various components and memory layout of \libname.
Processes within each group coordinate using the leader-follower model, like in Paxos and Protected Memory Paxos.
Each server process has a fixed-size buffer per client, analogously to other RDMA-based systems \cite{FaRM, Mu, DARE, APUS}.
A buffer in \libname is divided into two parts, a message buffer $M$, and a timestamp buffer $T$.
Message buffer $M$ is a shared memory region that can be read and written by any other processes, including the client process the buffer is associated with.
Timestamp buffer $T$ is protected and can only be written by the leader of each group; the buffer can be read by any processes.
Each entry in $M$, with a multicast message $m$, has a corresponding entry in $T$, with $m$'s timestamp.

%Each server process organizes its memory in regions: a shared
%memory region and a protected memory region. All processes have remote access
%(read/write) to the shared memory space. 
%Only one leader of each group
%has remote-write permission to a given node's log at any point in time
%during the protocol.

Clients keep a copy of the remote head and tail pointer of their buffer at each server. 
A client increases the remote tail after writing to the shared memory. 
The server process updates the head pointer on the client buffer after handling the message by piggybacking the new value in the response.
Each process $p$ periodically polls the memory cell at the head position of each
connected QPs to detect new messages.
%\libname maintain quorums of ``candidate'' processes of each group to be the leader of that group.




\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/architecture2}
  \caption{\libname's memory layout. The normal execution proceeds in three steps: in step 1, the client writes a multicast message in the memory of all destination processes; in step 2 the leader of each destination group proposes and writes a timestamp for the message in the memory of its followers and other leaders; in step 3 the leaders propagate the timestamp written by other leaders, and the followers acknowledge the proposed timestamps. The message is delivered after step 3.}
%Processes have shared and exclusive memory that can be accessed by RDMA primitives. Exclusive memory access needs permission. A client proposes new message by writing to the shared memory of all processes. Leader processes writes their proposed timestamps to exclusive memory. Follower processes write their acknowledgement to the shared memory}
  \label{fig:arch}
\end{figure}


\libname consists of the following main components:
\begin{itemize}
  \item \emph{Memory management.} Processes separate their memory into two regions: a shared buffer and a protected buffer (detailed in \S\ref{sec:ds-structs}). While all processes have access to read and write the shared buffer of a process, only the leader of each group has permission to write the protected buffer of a process. Each process grants and revokes write permission to its protected buffer, to ensure only one leader of each group has write access at a time.
  \item \emph{RDMA communication.} This component provides functions to read and write remote memory, used by the normal operation component, and to send and receive messages, used by the failure handling module.
  \item \emph{Normal execution.} The normal protocol execution (detailed in \S\ref{sec:normalcase}) is invoked when there is a sole leader per group with support of at least a majority of processes in its group. A leader is responsible for proposing the group's timestamp for the new message, and propagating timestamps from other groups to followers of its local group.
  \item \emph{Failure handling.} Upon detecting the failure of a leader, processes in a group elect a new leader. The new leader must ensure that its execution is consistent with the execution of the previous leader (detailed in \S\ref{sec:failurecase}).
  \item \emph{Leader election.} \libname requires processes to detect a slow or crashed leader, and elect a new leader. Leader election is not assumed to be perfect: the protocol ensures safety despite multiple leaders in a group. To ensure progress, though, eventually every group should elect a single operational and stable leader \cite{Aguilera2019,L98}.
Stable leader election can be implemented in the partially synchronous model \cite{Aguilera2001}.
\end{itemize}

\subsection{Data structures}
\label{sec:ds-structs}

Algorithm~\ref{alg:data_struct} presents the data structures used by processes in \libname.
Every server process has a shared buffer $M$ per client $c$, where entry $M[c,i]$ contains the $i$-th message $msg$ multicast by client $c$, the groups $dst$ the message is addressed to, and an address vector $ptr$, where $ptr[g,p]=j$ means that at process $p$ in group $g$ message $msg$ is stored in $M[c,j]$. 
The address vector is used by a process to know where to write in the memory of another process addressed by the message.
Servers compute the message's timestamp $tmp$, based on the timestamps proposed by the leader of each destination group, and the acknowledgements from the members of the leader's group, stored in vector $ack$.
%A timestamp is a tuple $(cnt,pid)$, where $cnt$ is a scalar and $pid$ is a process identifier, used to break ties.
A multicast message state $stat$ can be null ($\perp$), pending a final timestamp (\mcast), assigned a final timestamp (\ordered) or delivered (\done).

To compute a message's timestamp, each server process has a protected buffer $T$, where the $i$-th entry $T[c,i]$ matches the corresponding entry $M[c,i]$ in the shared buffer $M$ associated with $c$.
The entry contains a timestamp vector $tmp$ and a round vector $rnd$, each one with an entry per group $g$; $tmp[g]$ contains the timestamp proposed by the current leader in $g$ in round $rnd[g]$.
Timestamps and rounds are tuples $\langle cnt,pid \rangle$, where $cnt$ is a scalar and $pid$ is a process identifier. It follows that $\langle cnt,pid \rangle > \langle cnt',pid' \rangle$ iff $cnt > cnt'$, or $cnt = cnt'$ and $pid > pid'$.

To multicast a message, a client writes the message in the shared buffer of each process in the groups addressed by the message.
To know in which entry the multicast message must be written, the client keep an address vector $ptr$ with an entry per group and per process in the group.

Process $p$'s local state includes a $clock$, used to compute logical timestamps, $p$'s current round, used when $p$ is the leader of the group, vector $Leader$ with $p$'s view on the current leader of each group, and vector $Round$, where entry $Round[g]$ contains the largest round the process has accepted from $Leader[g]$.

\input{ds-structs}


%\begin{figure}[ht!]
%  \centering
%  \includegraphics[width=1\linewidth]{figures/memory}
%  \caption{Memory layout of \libname. Each process has shared and exclusive memory
%  space. All other processes can access shared memory. Only leader can access
%  exclusive memory }
%  \label{fig:normal_operation_time}
%\end{figure}







\subsection{Normal execution}
\label{sec:normalcase}


\input{algo-amcast}
% \input{algo-normal-case}

Algorithm \ref{alg:normal_case} presents \libname's normal execution, when a message is addressed to groups whose leaders are operational and stable.
Figure~\ref{fig:normal_operation_time} illustrates one normal execution.
We explain next the behavior of each one of the involved tasks.

\begin{itemize}
\item \emph{Task 1.} To multicast message $m$, client $c$ first calculates the next available entry in the buffer of every process addressed by $m$. Then, $c$ invokes the $Relay$ procedure, which copies the message, its destination, and the address vector for $m$ in the message buffer of every process addressed by $m$.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/execution}
  \caption{Normal execution of \libname. We show the steps at one follower per group only to avoid cluttering.}
  \label{fig:normal_operation_time}
\end{figure}

\item \emph{Task 2.} Once leader process $L$ in group $g$ reads $m$ in its message buffer, it computes a
group-wise timestamp for $m$ and writes the proposed timestamp 
in the protected timestamp buffer of all follower processes in the leader's group $g$, and all leader processes 
of other groups in the destination of $m$.
If a remote write is denied, then $L$ ends the task since another process in $g$ became leader.

\item \emph{Task 3.} When a follower detects that a multicast message has been assigned a timestamp by the group leader, it updates its clock, to ensure that timestamps from a group are monotonically increasing (necessary in case the process becomes leader).
Then, the follower acknowledges that it has read this timestamp by writing the round used by the leader in its entry in the $ack$ vector of every process in the destination of the message.
The follower detects the leader proposed timestamp by checking whether the round of the timestamp matches the round associated with the  leader. 
This assumes that both the timestamp and the round have been updated by the leader.
We discuss in Section~\ref{sec:implementation} how we ensure this in RDMA.

\item \emph{Task 4.} 
When the leader $L$ in $g$ reads a timestamp written by a leader in another group, $L$ updates its local clock with the timestamp, to ensure that any future proposed timestmaps will be bigger, and writes the read timestamp in the memory of each one of its followers.
This task is executed by the leader of a group only, since only the leader is updated with timestamps from the leader of another group (see Task 2).
The reason why only the leader of a group is updated is to ensure that any timestamps assigned by the leader are consistent with any other timestamps assigned by the group, similarly to other leader-follower-based protocols \cite{gotsman2019white, Junqueira2011}.
%Once each involved process $P$ belongs to $m.dst$ \lread message $m$ in its
%local buffer, $P$ mark the message as pending, and starts polling from it
%exclusive memory for timestamps of $m$ (Task 4 algorithm~\ref{alg:normal_case}).

\item \emph{Task 5.} 
When a message has a timestamp proposed by a leader from each destination group of the message, and a quorum of processes in each destination group agree with the proposed timestamp, the message become ordered.

\item \emph{Task 6.} 
A process delivers an ordered message when it can assert that no other message can be assigned a smaller timestamp.

\end{itemize}

\subsection{Handling failures}
\label{sec:failurecase}

%\input{algo-leader-election.tex}

When a process becomes a leader, it needs to catch up with the previous leader.
In the following, we describe how the newly elected leader does this.
The procedure uses both shared memory and message passing for communication.
In RDMA, message passing is less efficient than shared memory, but it reduces complexity, as we do not have to handle concurrent accesses to shared memory.
Since failures are hopefully rare, we consider that trading performance for simplicity is acceptable.

\begin{itemize}
\item \emph{Task 7.} 
When a process that will become the next leader of the group suspects the current leader, it determines its \emph{first undecided entry (FUE)} per client in its buffers. 
Then, the new leader chooses a round and sends a catch-up message to every process in the system.
Since, entry $i$ in the new leader's buffer may correspond to a different entry at another process, the new leader must convert its FUE into one that is meaningful for the contacted process.

\item \emph{Task 8.} 
A process will consider a catch-up message from new leader $q$ in group $h$ if $q$ has picked a round bigger than the current round for $h$ (this is a requirement from Paxos, to ensure that a new leader will not decide on a value different than a previously decided value).


\item \emph{Task 9.} 

\end{itemize}

Finally, we also consider the case of faulty clients, who may fail to update all message destinations.

\begin{itemize}

\item \emph{Task 10.} 
When a process detects the failure of a client, it relays all the messages multicast by the faulty client that have not been ordered yet, that is, only messages in the \mcast state need to be relayed.

\end{itemize}

%Firstly, the new leader must choose a new ballot number that is higher than all
%ballot numbers before for its group. A process only reply to the message or
%timestamp that has a ballot number higher than its current value. The new ballot
%number will be included in each timestamp $ts$ of this leader later.
%
%The new leader needs to get the access to the exclusive memory space on (1) the
%followers of its local group, and (2) the leaders of other groups. In order to
%act as a leader, a node must get the write permission from majority of other
%processes in its group. In addition, to tolerate failure of the leaders of other
%groups, the new leader also need to get the write permission from majority of
%processes of other groups. The new leader request the access by issuing a RDMA
%Send (\lle{this could be a \rwrite}) 1A message that includes a new ballot
%number all involved processes. Upon receiving such message, a process first
%checks if the proposed ballot number is higher than the current ballot number it
%stored. In such case, the process revokes access of the old leader, and replies
%to the new leader with an 1B message that also contains the status of all
%message it is processing (i.e., messages that are pending and messages that has
%been fulfilled and are waiting to be delivered)
%
%Once the new leader receives 1B reply from majority processes of each group, it
%must ensure all remote logs are up-to-date with its own. If a message m is
%fulfilled in some processes, the leader stores it in a settled list. If a
%message is not fulfilled in any process, there are two cases: i)  the message
%does not have the timestamp or enough acknowledgements from quorum of processes
%of the group that has failed leader on any process ii) it does have such data on
%some processes.\lle{sloppy sentence}

\input{algo-request-perm-v2}

% If the
% sequence number of leader is smaller than follower, the leader should know that
% for missing instances, it can read the agreed timestamps from followers.

% the
% sequence number of the most recent delivered message to all involved processes.
% When a process receives this message, it checks if the ballot number is higher
% than its current ballot number, then revokes access of the old leader, and send a
% reply to the new leader with its current sequence number.



% Finally, once the new leader receives reply from majority processes, it must
% ensure all remote logs are up-to-date with its own. If the sequence number of
% leader is smaller than follower, the leader should know that for missing
% instances, it can read the agreed timestamps from followers.

% With the pending instances (the instances in the pending list without
% timestamps, or has not received majority of acks), leader can just rewrite new
% timestamp value with its new ballot number. In this case leader issues a RDMA
% WRITE-WITH-IMM which triggers a completion event at the receivers side.


% not necessary
% if the sequence number of leader is bigger than follower, the follower can do a
% \rread of missing timestamps from leader's memory.

\subsection{Extensions}
\label{sec:extensions}

How to deliver single-group messages more quickly, after the write from the leader of the group

How to garbage collect entries in the shared buffers
