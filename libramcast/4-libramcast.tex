%!TEX root =  main.tex

\section{RamCast: RDMA-based Atomic Multicast}
\label{sec:rdma-atomic-multicast}

In this section, we explain the general design and architecture of \libname (\S\ref{sec:overview}), then detail \libname's atomic multicast algorithm in the normal case without failures (\S\ref{sec:normalcase}), and in the presence of failures (\S\ref{sec:failurecase}).

\subsection{Building blocks}
\label{sec:overview}

\libname leverages two ideas, Skeen's folklore atomic multicast algorith \cite{BJ87b} and Protected Memory Paxos \cite{Aguilera2019}.
Skeen's algorithm orders messages multicast to multiple processes consistently but it does not tolerate failures.
Protected Memory Paxos takes advantage of RDMA permissions to improve the efficiency of Paxos \cite{L98}. 
Like Paxos, it implements atomic broadcast, that is, it assumes a single group of processes.
Combining these ideas coherently and efficiently is not trivial, as we discuss in this section,
after briefly presenting Skeen's algorithm and Protected Memory Paxos.

%\subsubsection{Skeen's atomic multicast}

In Skeen's algorithm, each process and assigns timestamps to multicast messages based on a logical clock~\cite{Lam78}.
The correctness of the algorithm stems from two basic properties:
(i)~processes in the destination of a multicast message first assign tentative timestamps to the message and eventually agree on the message's final timestamp; and
(ii)~processes deliver messages according to their final timestamp.
These properties are implemented as follow.

\begin{itemize}
\item[(i)] To multicast a message $m$ to a set of processes, $p$ sends $m$ to the destinations.
Upon receiving $m$, each destination updates its logical clock, assigns a tentative timestamp to $m$, stores $m$ and its timestamp in a buffer, and sends $m$'s timestamp to all destinations.
Upon receiving timestamps from all destinations in $m.dst$, a process computes $m$'s final timestamp as the maximum among all received tentative timestamps for $m$.
\item[(ii)]Messages are delivered respecting the order of their final timestamp.
A process $p$ delivers $m$ when it can ascertain that $m$'s final timestamp is smaller than the final timestamp of any messages $p$ will deliver after $m$ (intuitively, this holds because logical clocks are monotonically increasing).
\end{itemize}

%\subsubsection{Protected Memory Paxos}

In Paxos, in order for the leader to order a message $m$, it assigns $m$ to a consensus instance and proposes that $m$ be accepted in the proposed instance.
In the normal case, where there is a single leader, the followers accept the proposed message and reply to leader.
Protected Memory Paxos optimizes this procedure by granting to the leader exclusive write permission to the memory of the followers.
If another process becomes leader, then it takes the exclusive permission.
The idea is that the leader writes the proposed message in the memory of the followers.
If the leader succeeds to write the message in the memory of a quorum of followers, then it is because no other leader took over.
This happens because if another leader took over, then it has taken the permission of the original leader.

\subsection{Overall design and architecture}

Figure~\ref{fig:arch} depicts the architecture of \libname.
Processes within each group coordinate using the leader-follower model, like in Paxos and Protected Memory Paxos.
Each server process has a fixed-size circular buffer per client, analogously to other RDMA-based systems \cite{FaRM, Mu, DARE, APUS}.
A circular buffer in \libname is divided into two parts, a message buffer $M$, and a timestamp buffer $T$.
Message buffer $M$ is a shared memory region that can be read and written by any other processes, including the client process the buffer is associated with.
Timestamp buffer $T$ is protected and can only be written by the leader of each group; the buffer can be read by any process.
Each entry in $M$, with a multicast message $m$, has a corresponding entry in $T$, with $m$'s timestamp.

%Each server process organizes its memory in regions: a shared
%memory region and a protected memory region. All processes have remote access
%(read/write) to the shared memory space. 
%Only one leader of each group
%has remote-write permission to a given node's log at any point in time
%during the protocol.

Clients keep a copy of the remote head and tail pointer of their buffer at each server. 
A client increases the remote tail after writing to the shared memory. 
The server process updates the head pointer on the client buffer after handling the message by piggybacking the new value in the response.
Each process $p$ periodically polls the memory cell at the head position of each
connected QPs to detect new messages.
%\libname maintain quorums of ``candidate'' processes of each group to be the leader of that group.




\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/architecture}
  \caption{Architecture of \libname.}
%Processes have shared and exclusive memory that can be accessed by RDMA primitives. Exclusive memory access needs permission. A client proposes new message by writing to the shared memory of all processes. Leader processes writes their proposed timestamps to exclusive memory. Follower processes write their acknowledgement to the shared memory}
  \label{fig:arch}
\end{figure}


\libname consists of the following main components:
\begin{itemize}
  \item \emph{Leader election.} Processes detect failures of leaders and
  selects other replicas to become leader.
  \item \emph{Memory management.} Processes separate their shared buffer into two
  regions: shared memory space and exclusive memory space. Follower processes
  participate in consensus by writing to the shared memory space. Leader
  processes write their proposal in the exclusive memory space.
  \item \emph{Replication.} .
  \item \emph{Leader permission management.} Each process grants and revokes
  write permission to its shared buffer of the remote leader, to ensure only one
  single leader of a group has write access at a time.
  \item \emph{RDMA communication.} This component provides functions to read and
  write data in remote replicas.
\end{itemize}

\subsection{Data structures and memory layout}


%\input{ds-structs}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/memory}
  \caption{Memory layout of \libname. Each process has shared and exclusive memory
  space. All other processes can access shared memory. Only leader can access
  exclusive memory }
  \label{fig:normal_operation_time}
\end{figure}







\subsection{Normal case execution}
\label{sec:normalcase}


\input{algo-amcast}
% \input{algo-normal-case}

The normal operation protocol is involved when there is an existence of a sole
leader per group that has the support of at least a majority of processes of
that group. A leader is responsible for proposing timestamp for new message, and
propagating timestamps from other groups to followers of its local group.

A client process $C$ multicasts a message $m$ to a set of target groups in
$m.dst$ by performing a \rwrite to the same memory entry on the shared memory of
each process $p_i$ belongs to $m.dst$. (Task 1 algorithm~\ref{alg:normal_case})

\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{figures/timeline-simple}
  \caption{Normal case execution of \libname. Step 1: client writes message to all
          involves processes. Step 2: leader of each group in the destination
          propose and write a timestamp to all followers and other leaders
          memory. Step 3: leaders propagate the timestamp written by other
          leaders, followers write their acks for local timestamp on all other
          processes}
  \label{fig:normal_operation_time}
\end{figure}

Once leader process $L$ in group $g$ \lread the message $m$, it computes a
group-wise timestamp $ts$ and writes that proposed timestamp $ts$ to the memory
cell reserved for $m$ in the exclusive memory space of: (1) all follower
processes $f$ belongs to $g$. (2) all leader processes $L_i$ of other groups
$g_i$ belongs to $m.dst$ The proposed timestamp $ts$ consists of three
components: the ballot number $bal$ in which $L$ is the leader of group $g$, the
sequence number $sel$ is the consensus instance of message $m$ in group $g$, and
$v$ is $L$'s logical clock value. (Task 2 algorithm~\ref{alg:normal_case})

Upon \lread a timestamp $ts$ of message $m$ from leader processes of other
groups, a leader $L$ propagates that timestamp to its group by performing
\rwrite $ts$ to the exclusive memory space for $m$ in all follower processes of
its local group. (Task 3 algorithm~\ref{alg:normal_case})

Once each involved process $P$ belongs to $m.dst$ \lread message $m$ in its
local buffer, $P$ mark the message as pending, and starts polling from it
exclusive memory for timestamps of $m$ (Task 4 algorithm~\ref{alg:normal_case}).

Upon \lread a timestamp $ts$ of message $m$ from leader of local group, a
process $P$ writes its acknowledgement $ack$ for that timestamp to the shared
memory of all other involved processes, and start polling from its memory for
acknowledgements from other processes. (Task 4 algorithm~\ref{alg:normal_case}).
Once $P$ receives timestamps of all involved groups and acknowledgements from
the majority of processes of each group, it can order the message $m$ and starts
deliver it.

After delivering a message, a process will keep that message in its local buffer.
Upon receiving acknowledgements from all the processes in the candidate
quorum, the message will be removed from the buffer.

\subsection{Handling failures}
\label{sec:failurecase}

%\input{algo-leader-election.tex}

\libname assumes the existence of an unreliable failure detector. If a leader is
slow or crashes, the failure detector will detect this and elect new leader from
the candidate quorums. When a process becomes a leader, it needs to perform
several steps to ensure it is the only leader of its groups. Those steps are:
(1) increasing ballot number, in which that process becomes leader. (2) sending
1A message to all processes with new ballot number, and wait for the response of
majority (3) ensure the logs of all processes are up-to-date

Firstly, the new leader must choose a new ballot number that is higher than all
ballot numbers before for its group. A process only reply to the message or
timestamp that has a ballot number higher than its current value. The new ballot
number will be included in each timestamp $ts$ of this leader later.

The new leader needs to get the access to the exclusive memory space on (1) the
followers of its local group, and (2) the leaders of other groups. In order to
act as a leader, a node must get the write permission from majority of other
processes in its group. In addition, to tolerate failure of the leaders of other
groups, the new leader also need to get the write permission from majority of
processes of other groups. The new leader request the access by issuing a RDMA
Send (\lle{this could be a \rwrite}) 1A message that includes a new ballot
number all involved processes. Upon receiving such message, a process first
checks if the proposed ballot number is higher than the current ballot number it
stored. In such case, the process revokes access of the old leader, and replies
to the new leader with an 1B message that also contains the status of all
message it is processing (i.e., messages that are pending and messages that has
been fulfilled and are waiting to be delivered)

Once the new leader receives 1B reply from majority processes of each group, it
must ensure all remote logs are up-to-date with its own. If a message m is
fulfilled in some processes, the leader stores it in a settled list. If a
message is not fulfilled in any process, there are two cases: i)  the message
does not have the timestamp or enough acknowledgements from quorum of processes
of the group that has failed leader on any process ii) it does have such data on
some processes.\lle{sloppy sentence}

\input{algo-request-permission}

% If the
% sequence number of leader is smaller than follower, the leader should know that
% for missing instances, it can read the agreed timestamps from followers.

% the
% sequence number of the most recent delivered message to all involved processes.
% When a process receives this message, it checks if the ballot number is higher
% than its current ballot number, then revokes access of the old leader, and send a
% reply to the new leader with its current sequence number.



% Finally, once the new leader receives reply from majority processes, it must
% ensure all remote logs are up-to-date with its own. If the sequence number of
% leader is smaller than follower, the leader should know that for missing
% instances, it can read the agreed timestamps from followers.

% With the pending instances (the instances in the pending list without
% timestamps, or has not received majority of acks), leader can just rewrite new
% timestamp value with its new ballot number. In this case leader issues a RDMA
% WRITE-WITH-IMM which triggers a completion event at the receivers side.


% not necessary
% if the sequence number of leader is bigger than follower, the follower can do a
% \rread of missing timestamps from leader's memory.
