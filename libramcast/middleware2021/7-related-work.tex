%!TEX root =  main.tex
\section{Related work}
\label{sec:related-work}

\libname is at the intersection of atomic multicast protocols (\S\ref{sec:rwamcast}), 
RDMA-based systems (\S\ref{sec:rwrdmasys}), and RDMA-based consensus protocols (\S\ref{sec:rwrdmacons}).


\subsection{Atomic multicast}
\label{sec:rwamcast}

Atomic multicast is a well-studied problem. Skeen's algorithm (described in \S\ref{sec:bblocks}) is possibly the first atomic multicast algorithm.
%Processes in Skeen assign timestamps to messages ensure that destinations agree
%on the final timestamp assigned to each message, and deliver messages following
%this timestamp order. The precise way in which these properties are ensured
%varies from one algorithm to another. 
Even though it is not fault-tolerant, it
is genuine: processes only communicate if they are in the destinations of the messages. 
Later timestamp-based genuine atomic multicast algorithms 
implemented fault-tolerant versions of Skeen's protocol. 
FastCast \cite{Coelho2017} speeds up the delivery of messages by overleaping some parts of the protocol (i.e., the order proposed by the leader and the consensus needed to decide on the proposed order).
In good runs, FastCast delivers multi-group messages in 4 communication steps.
White-Box Atomic Multicast \cite{gotsman2019white} further improves 
latency with a protocol that combines Paxos and a fault-tolerant version of Skeen's protocol.
White-Box Atomic Multicast delivers multi-group messages in 3 communication steps at the leaders of the involved groups and 4 communication steps at the followers.
\libname improves on White-Box Atomic Multicast in that both leaders and followers can deliver a multi-group message in 3 communication steps.

Ring-based protocols \cite{delporte2000fault, bezerra2015ridge,
marandi2012multi} proposed a different approach to high throughput by
propagating messages along a predefined ring overlay and ensuring atomic multicast
properties by relying on this topology. However, ring-based algorithms are
non-genuine: involved processes communicate with processes outside the
destination groups to deliver messages. The time complexity of these algorithms is
proportional to the number of destination groups.

\subsection{RDMA systems}
\label{sec:rwrdmasys}

Remote Direct Memory Access (RDMA) \cite{kalia2016design} is an interface that
allows servers to read and write the memory of a remote server directly. Over
the years, RDMA has become an active area of research for its high throughput,
low latency, and low CPU overhead. RDMA techniques have been implemented in
various architectures, including Infiniband \cite{pfister2001introduction}, RoCE
\cite{beck2011performance}, and iWRAP \cite{rashti200710}. RDMA has already been
explored and applied in a variety of applications, from key/value stores
\cite{FaRM, kalia2014using, mitchell2013using, wei2015fast}, to databases
\cite{binnig2015end, huang2019rdma}, and distributed file systems
\cite{islam2012high, li2009early, wu2003pvfs}. Pilaf \cite{mitchell2013using} is
a distributed in-memory key-value store that implements client-lookup operations
with one-sided RDMA reads. In contrast, in HERD \cite{kalia2014using}, clients use
one-sided RDMA writes to send requests to servers which poll their receive RDMA
buffers to process requests. FaRM \cite{FaRM} proposes a distributed computing
platform, which provides the transactional interface for applications to access
the shared memory. NVFS \cite{islam2012high} provides a novel design of HDFS
with byte-addressable NVM and RDMA network. Octopus \cite{lu2017octopus} is a
distributed, shared persistent memory file system that combines RDMA and NVM's
new features by redesigning the software. Besides, many optimization guidelines
were proposed by Kalia et al. \cite{kalia2016design} to enhance performance of
RDMA system. 
We have applied many of the mentioned best practices
in the implementation of \libname.


\subsection{RDMA-based consensus}
\label{sec:rwrdmacons}

RDMA has received limited attention in the context of consensus
protocols, and only a few crash-tolerant replication protocols based on RDMA 
have been proposed. DARE \cite{DARE} aims to optimize for low latency in replica
communication. The consensus leader in DARE replicates requests to its follower
with RDMA one-sided read/write operations, and makes use of permissions when
changing leaders. APUS \cite{APUS} improves upon DARE. APUS
combines RDMA with Paxos and focuses on scalability with the number of
connections and replicas. APUS is based on intercepting inbound socket calls, so
it does not require modifying applications for integration. Derecho
\cite{jha2019derecho} provides an RDMA-enabled state machine replication while
structuring applications into groups, subgroups, and shards. Derecho only needs
a leader for group reconfiguration. 
Mu \cite{Mu} exploits Protected Memory Paxos and colocates the client with the leader of Paxos to reach low latency.
Similarly to Mu, \libname also relies on RDMA's protected memory to order single-group messages efficiently.
However, \libname cannot colocate clients and leaders since it implements atomic multicast.
This happens because clients may multicast a message to different groups and colocating all leaders in the same host defeats the purpose of atomic multicast.

%However, these protocols focus on building consensus on top of RDMA primitives.
%To our knowledge, there is no previous work that solved atomic multicast with
%RDMA.


% \section{State machine replication}

% State machine replication (SMR) was first introduced by Lamport in \cite{Lam78}. Schneider \cite{Sch90} then presented a more systematic
% approach to the design and implementation of SMR protocols. Since then, SMR has
% become a well-known approach to replication and has been extensively studied,
% both in academia (e.g., \cite{Kapritsos:2012um, Kotla:2004ep, santos2013htsmr,
% Mencius}) and in the industry (e.g., \cite{corbett2013spanner}). SMR provides strong consistency guarantees, which
% come from total order and deterministic execution of commands. Traditional SMR
% relies on a consensus protocol to define a common order among replicas for the execution of 
% commands. Deterministic execution is usually ensured by having every replica
% execute the set of ordered commands sequentially.

% % 
