\section{Experimental Evaluation}
\label{sec:experimental-evaluation}

In this section, we present the experimental results of \libname, comparing to Kernel Paxos 
%, an implementation of Multi-Paxos that reduces communication overhead by avoiding system calls and TCP/IP stack \cite{esposito2018kernel} 
and WbCast. % \cite{gotsman2019white}, another genuine atomic multicast protocol that takes as low as 3 message delays to deliver a message.
In Section~\ref{sec:evaluation:setup}, we describe the environment where we conducted our experiments and the parameters given to the different protocols.
In Section~\ref{sec:evaluation:micro}, we show the micro-benchmark results with different message sizes.
In Section~\ref{sec:evaluation:broadcast}, we exhibit the results for broadcast (i.e., multicast with a single group).
In Section~\ref{sec:evaluation:multicast}, we show throughput scales with the number of multicast groups for each protocol.


\subsection{Environment setup and configuration parameters}
\label{sec:evaluation:setup}

We conducted all experiments using the CloudLab infrastructure~\cite{DuplyakinATC19cloudlab} with two sets of nodes: 
(a) R320 node for broadcast experiments, equipped with one eight-core Xeon E5-2450 processor running at 2.1GHz, 16 GB of main memory, and a Mellanox FDR CX3 NIC; and (b) XL170 nodes for other experiments, equipped with one ten-core Intel E5-2640v4 processor running at 2.4GHz, 64 GB of main memory, and a Mellanox ConnectX-4 NIC. 
A 10 Gbps network link with around 0.1ms round-trip time connects all nodes running Ubuntu Linux 18.04 with kernel 4.15 an Oracle Java SE Runtime Environment 11. 

In all our experiments, there are client and server processes. 
Clients send requests in a closed-loop, i.e., each client multicasts a message to servers and waits for a response before multicasting the next message. 
In every protocol, each group has 3 processes with in-memory storage.

We defined three experimental setups:
in~\S\ref{sec:evaluation:micro} we deployed \libname with a single group and increasing message size;
\S\ref{sec:evaluation:broadcast} compares \libname with Kernel Paxos and WbCast in a broadcast (single-group) deployment;
\S\ref{sec:evaluation:multicast} presents a scenario with eight groups contrasting \libname and WbCast with an increasing number of clients and destination groups.

\subsection{Micro-benchmark: increasing message size}
\label{sec:evaluation:micro}
In this experiment, conducted on XL170 nodes, we measure \libname throughput and latency for different message sizes.
For each message size, from 64 bytes to 64 Kilobytes, we increase the number of clients until the system is saturated, i.e., throughput stops improving while latency raises.
Figure~\ref{fig:1group_message_size} (top) shows that messages up to 4KB long do not impact the overall system throughput, with nearly 250 thousand messages delivered each second. 
As the message size increases, the maximum throughput decreases with 35 thousand messages per second for 64KB messages.
The latency cumulative distribution function (CDF) in Figure~\ref{fig:1group_message_size} (bottom) exhibits minimum latency variation for messages with up to 4KB, around 20 microseconds.

\begin{figure}[htp!]
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/benchmark/graphs/figure-performance-vs-size-single-group-up-to-4k}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/benchmark/graphs/figure-performance-vs-size-single-group-cdf-up-to-4k}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/benchmark/graphs/figure-performance-vs-size-single-group-from-4k}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/benchmark/graphs/figure-performance-vs-size-single-group-cdf-from-4k}
  \end{subfigure}
  \caption{\libname performance with different message sizes: throughput versus latency (top) and latency cumulative distribution function measured when the system is saturated (bottom)}
  \label{fig:1group_message_size}
\end{figure}

\subsection{Broadcast results}
\label{sec:evaluation:broadcast}

This experiment assesses how each protocol behaves in a scenario with a single group of three replicas for 64-byte and 1-kilobyte messages.
Figure~\ref{fig:broadcast} (top) shows that, for 64-byte messages, \libname outperforms the competitors with approximately 200 thousand messages per second. 
Kernel Paxos comes close with 170 thousand messages per second, while WbCast saturates sooner, with less than 60 thousand messages per second.
For larger messages, \libname kept the same performance, while the other protocols had the throughput reduced by half.
Figure~\ref{fig:broadcast} (bottom) confirms the superior results for \libname. The median latency for WbCast with small messages and a single client is nine times greater than the same measurement for our protocol.

\begin{figure}[htp!]
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/benchmark/graphs/figure-compare-single-group-throughput}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/benchmark/graphs/figure-compare-single-group-latency-cdf}
  \end{subfigure}
  \caption{Broadcast performance of \libname and competitors: throughput (top) and latency cumulative distribution function for a single client (bottom)}
  \label{fig:broadcast}
\end{figure}


\subsection{Multicast results: performance vs. number of destinations}
\label{sec:evaluation:multicast}

The last set of experiments assess \libname behavior in scenarios with up to 8 groups of 3 replicas each, deployed on XL170 machines.
The first experiment comprises executions in which clients send single-group 64-byte messages in setups with 1, 2, 4, and 8 groups.
Figure~\ref{fig:multicast-single-group} (top) shows the aggregated throughput results when the system is saturated. 
\libname outperforms WbCast by a large margin with 227  and 63 thousand messages per second, respectively, for 1 group, and around 1800 and 500 thousand messages per second for 8 groups.
These results also enhance the benefit of genuine atomic multicast algorithms and show that the throughput grows linearly with the number of groups for single-group messages. 
Since groups do not exchange any information when dealing with single-group messages, the latency CDF is the same for a single client, no matter the number of groups in the system, as depicted in Figure~\ref{fig:multicast-single-group} (bottom).

\begin{figure}[htp!]
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/benchmark/graphs/figure-genuine-compare-throughput}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/benchmark/graphs/figure-genuine-compare-latency-cdf}
  \end{subfigure}
  \caption{Multicast performance of \libname and WbCast for single group messages: throughput (top) and latency cumulative distribution function of a \libname's single client (bottom)}
  \label{fig:multicast-single-group}
\end{figure}

The next experiment evaluates how the protocols behave regarding multi-group messages. 
In setups with 1, 2, 4, and 8 groups, clients send 64-byte messages addressed to all the groups.
\libname's maximum throughput is greater than WbCast's in every configuration with 233, 145, 80, and 40 thousand messages per second for 1, 2, 4, and 8 destination groups against 63, 50, 35, and 27 thousand for WbCast, as shown in Figure~\ref{fig:multicast-multi-group} (top).
The difference is even more expressive if we consider the latency for a single client, i.e., when both protocols are contention-free. Figure~\ref{fig:multicast-multi-group} (middle) shows that the latency CDF for \libname with values of 23, 46, 78 and 150~microseconds for 1, 2, 4, and 8 destination groups if we consider the $95^{th}$ percentile. 
The equivalent values for WbCast are 214, 445, 673, and 1055~microseconds, representing 10 to 7 times slower delivery times when compared to \libname's.

\begin{figure}[htp!]
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/benchmark/graphs/figure-multi-dest-compare-throughput}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/benchmark/graphs/figure-multi-dest-compare-latency-cdf-ramcast}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/benchmark/graphs/figure-multi-dest-compare-latency-cdf-wbcast}
  \end{subfigure}
  \caption{Performance comparison of \libname and WbCast for multi-groups message: throughput (top) and latency cumulative distribution function of \libname's (middle) and WbCast's single client (bottom)}
  \label{fig:multicast-multi-group}
\end{figure}
