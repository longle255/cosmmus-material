











automata









labelfont=sl,sf

algebra
morekeywords=import,sort,constructors,observers,transformers,axioms,if,
else,end,
sensitive=false,
morecomment=[l]//s,


Scaling State Machine Replication 

Long Hoang Le 
Fernando Pedone 

Yesterday 
September 
2009 
Lugano 
The PhD program Director pro tempore 



  Langheinrich MarcUniversita della Svizzera Italiana, Switzerland
  Soulé RobertUniversita della Svizzera Italiana, Switzerland
  Correia MiguelUniversidade de Lisboa, Portugal
  Guerraoui RachidÉcole polytechnique fédérale de Lausanne, Switzerland
  Wojciechowski PawełPoznań University of Technology, Poland


To my beloved 
Someone said Someone 



































   
 

















numbers











  State machine replication (SMR) is a well-known technique that guarantees
  strong consistency (i.e., linearizability) to online services. However, SMR
  lacks scalability since every replica executes all commands, which limits the
  overall performance by the throughput of a single replica, so adding servers
  does not increase the maximum throughput. Scalable SMR () addresses this
  problem by partitioning the service state, allowing commands to execute only
  in some replicas and providing scalability while still ensuring
  linearizability. One problem is that only works with static partitioning
  scheme, and quickly saturates when executing multi-partition commands, as
  partitions must communicate. In this research, we propose , a novel
  approach to scaling SMR. uses dynamically repartitioning state
  technique, combined with a centralized oracle to maintain a global view of
  workload and inform heuristics about data placement. Using this oracle,
  is able to adapt to workload changes over time, while also
  minimizing the number of state changes. The preliminary results show that
  is a practical technique that achieves excellent throughput.




  [1-2]



[Preface]
  The result of this research appears in the following publications:
  [1-2]









[Introduction]Introduction

Context and goal of the thesis

Moderns applications today deploy their service across a massive and
continuously expanding infrastructure, in order to serve their users efficiently
and reliably. With the wide availability of commodity hardware, failures in data
centers are increasingly common and inevitable, even in well-established service
providers disruption:google. The causes of failures are varied. Machines
fail individually due to power failures, hardware failures, or collectively due
to human error disruption:amazon, software bugs, or even extreme weather
events disruption:weather. While confronting such failures, applications
still need to offer uninterrupted service to their users. To this end,
introducing fault tolerance through redundancy is critical for the availability
and integrity of the application.

Redundancy by replication can increase both availability and scalability. By
having several copies of the application running on multiple replicas, the
failure of one replica does not result in the interruption of the service.
Moreover, requests from users can be distributed across multiple replicas, so
the workload can be divided among different machines, which would translate into
better scalability. A main difficulty with replicating an application though is
managing consistency among replicas. A strong consistency system gives
programmers an intuitive model for the effects of concurrent updates (i.e.,
clients perform concurrent updates as there is a single copy of the application
state), making it less likely that complex system behavior will result in bugs.

State-machine replication is a well-known form of software replication, in
particular of active replication to building fault-tolerant services
using commodity hardware. (e.g.,
). In essence,
the idea is to model the application as a deterministic state machine whose
state transitions consist of the execution of client requests 
. Then the service is fully replicated on several servers and
deterministically execute every client command in every non-faulty server in the
same order to reach the same results. State machine replication provides
configurable fault tolerance in the sense that the system can be set to tolerate
any number of faulty replicas. In terms of scalability, unfortunately, classic
state machine replication does not scale. Increasing the number of replicas will
not scale performance since each replica must execute every command. Thus the
overall performance is limited by the throughput of a single replica.

Conceptually, scalable performance can be achieved with state partitioning
(e.g., ). Ideally, if the
service state can be divided such that commands access one partition only and
are equally distributed among partitions, then system throughput (i.e., the
number of commands that can be executed per time unit) will increase linearly
with the number of partitions. Although promising, exploiting partitioning in
SMR is challenging.
First, most applications cannot be partitioned in such a way that commands
always fall within a single partition. Therefore, a partitioning scheme must
cope with multi-partition commands.
Second, determining an efficient partitioning of the state that avoids load
imbalances and favors single-partition commands normally is computationally
expensive and requires an accurate characterization of the workload. Even if
enough information is available, finding a good partitioning is a complex
optimization problem .
Third, many online applications experience variations in demand. These happen
for a number of reasons. In social networks, some users may experience a surge
increase in their number of followers (e.g., new ``celebrities"); workload
demand may shift along the hours of the day and the days of the week, and
unexpected (e.g., a video that goes viral) or planned events (e.g., a new
company starts trading in the stock exchange) may lead to exceptional periods
when requests increase significantly higher than in normal periods. 
ideal partitioning scheme should be able



It is crucial that highly available partitioned systems be able to dynamically
adapt to the workload, since many online applications experience variations in
demand. We believe there should be a way to provide strong consistency in a
scalable manner, that is, to create a linearizable, scalable and efficient
replicated service. Throughout this thesis, we try to solve all problems
mentioned above, while substantiating the following claim:


"It is possible to devise an approach that allow a fault-tolerant system to
scale by dynamically adapting to the changes in the workload, while providing
strong consistency, that is, ensuring linearizability"



Research contributions



The contribution of this thesis is centered on scaling performance of a
replicated state machine system. In this section, we outline the main
contributions of this work and provide a short description of each one. We defer
detailed discussions to the next chapters.





To achieve the goal defined above for the doctoral thesis, we first propose
 (), a technique that allows a partitioned SMR system
to reconfigure its data placement on-the-fly.  achieves dynamic data
reconfiguration without sacrificing scalability or violating the properties of
classical SMR. These requirements introduce significant challenges. Since state
variables may change location, clients must find the current location of
variables. The scalability requirement rules out the use of a centralized oracle
that clients can consult to find out the partitions a command must be multicast
to. Even if clients can determine the current location of the variables needed
to execute a command, by the time the command is delivered at the involved
partitions one or more variables may have changed their location. Although the
client can retry the command with the new locations, how to guarantee that the
command will succeed in the second attempt? In classical SMR, every command
invoked by a non-faulty client always succeeds.  should provide similar
guarantees.  was designed to exploit workload locality. This scheme
benefits from simple manifestations of locality, such as commands that
repeatedly access the same state variables, and more complex manifestations,
such as structural locality in social network applications, where users with
common interests have a higher probability of being interconnected in the social
graph. Focusing on locality allows us to adopt a simple but effective approach
to state reconfiguration: whenever a command requires data from multiple
partitions, the variables involved are moved to a single partition and the
command is executed against this partition. To reduce the chances of skewed load
among partitions, the destination partition is chosen randomly. Although 
could use more sophisticated forms of partitioning, formulated as an
optimization problem (e.g., ), its technique has
the advantage that it does not need any prior information about the workload and
is not computationally expensive.

 addresses the limitations of  by adapting the partitioning
scheme as workloads change, by moving data ``on demand'' to maximize the number
of single partition user commands, while avoiding imbalances in the load of the
partitions. The major challenge in this approach is determining how the system
selects the partition to which to move data.  selects partitions
randomly, which allows for a completely decentralized implementation, in which
partitions make only local choices about data movement. We refer to this
approach as decentralized partitioning. This approach works well for data
with strong locality, but it is unstable for workloads with weak
locality 



.
This happens because with weak locality, objects in  are constantly
being moved back and forth between partitions without converging to a stable
configuration.

For this reason, we introduce the main contribution of this thesis, ,
Optimized Dynamic Partitioning for Scalable State Machine Replication. Like the
other approach, does not require any a priori knowledge about the
workload. However, differs from the prior approach because it creates
the workload graph on-the-fly and use graph partitioning techniques to
efficiently relocate application state on-demand. In order to reach the
optimized partitioning,  maintains a location oracle with a global
view of the application state. The oracle minimizes the number of state
relocations by monitoring the workload, and re-computing an optimized
partitioning on demand using a static partitioning algorithm. The oracle is also
the first contact when a client submits a command, to discover the partitions on
which state variables are stored. If the command accesses variables in multiple
partitions, the oracle issues a move command to the partitions, re-locating
variables to a single partition. Of course, when re-locating a variable, the
oracle is faced with a choice of which partition to use as a destination.
chooses the partition for relocation (i.e., one that would minimize
the number of state relocations) by partitioning the workload graph using the
METIS  partitioner. To track object locations without
compromising scalability, in addition to information about the location of state
variables in the oracle, each client caches previous consults to the oracle. As
a result, the oracle is only contacted the first time a client accesses a
variable or after a variable changes its partition. Under the assumption of
locality, we expect that most queries to the oracle will be accurately resolved
by the client's cache.

Thesis outline

The rest of the thesis is organized as follows. Chapter 2 provies the background
for the thesis, by describing the system model, defines the communication
primitives used throughout this thesis. Chapter 3 studies the problem of scaling
state machine replication from a mostly theoretical perspective. In Chapter 4 we
discuss , an dynamic partitioning scheme for , explaining how it may
achieve dynamic state reconfiguration for while still maintaining strong
consistency. In Chapter 5, we present , which combines the dynamic
repartitioning state technique and the centralized oracle to maintain a global
view of workload and partition application state. Chapter 6 presents some
related works on scaling state machine replication. Finally, Chapter 7 concludes
this thesis and proposes future research directions.


[System Model and Definitions]System Model and Definitions


In this chapter, we present the system model, introduce two variations of a
multicast communication primitive, and define our correctness criterion (i.e.,
linearizability).

System model


  Processes and communication. We consider a distributed system
  consisting of an unbounded set of client processes 
  and a bounded set of server processes (replicas) .
  Set  is divided into disjoint groups of servers .
  Processes communicate by message passing, using either one-to-one or
  one-to-many communication. One-to-one communication uses primitives
   and , where  is a message and  is the process
   is addressed to. If sender and receiver are correct, then every message
  sent is eventually received. One-to-many communication relies on reliable
  multicast and atomic multicast, defined in sec:rmcast and
  sec:amcast, respectively.
  Failure model. We assume the crash failure model.
  Processes are either correct, if they never fail, or faulty,
  otherwise. In either case, processes do not experience arbitrary behavior
  (i.e., no Byzantine failures).
  Synchrony model. It is impossible to solve consensus in an
  asynchronous system . So we consider a system that is
  partially synchronous : it is initially asynchronous and
  eventually becomes synchronous. When the system is asynchronous, there are no
  bounds on the time it takes for messages to be transmitted and actions to be
  executed; when the system  is synchronous, such bounds exist but are unknown
  to the processes. The partially synchronous assumption allows consensus, a
  fundamental problem at the core of replication , to be
  implemented under realistic conditions .


Reliable multicast


Reliable broadcast provides stronger properties than regular broadcast, by
ensuring that a message is either delivered to all processes or to none. To
reliably multicast a message  to a set of groups , processes use
primitive .  Message  is delivered at the destinations
with .  Reliable multicast has the following properties:



    [--] If a correct process s , then every correct process in
       s  (validity).

    [--] If a correct process s , then every correct process in
       eventually s  (agreement).

    [--] For any message , every process  in  s 
      at most once, and only if some process has   to 
      previously (integrity).



Atomic multicast

Atomic broadcast, also referred to as total order broadcast, is an extension of
reliable broadcast that ensures that messages are delivered by all processes in
the same order. To atomically multicast a message  to a set of groups
, processes use primitive .  Message  is
delivered at the destinations with .  We define delivery order  as
follows:  iff there exists a process that delivers  before .

Atomic multicast ensures the following properties:



    [--] If a correct process s , every correct process in a
      group in  s  (validity).

    [--] If a process s , then every correct process in a group
      in  s  (uniform agreement).

    [--] For any message , every process s  at most once, and
      only if some process has   previously (integrity).

    [--] If a process s  and then , then no process
    s  before  (fifo order).

    [--] The delivery order is acyclic (atomic order).

    [--] For any messages  and  and any processes  and  such
      that ,  and , if 
      delivers  and  delivers , then either  delivers  before
       or  delivers  before  (prefix order).



Atomic broadcast is a special case of atomic multicast in which there
is a single group of processes.

Consensus
[1-2]

Linearizability
[1-2]

Replication
[1-2]

State-machine replication


State machine replication is a fundamental approach to implementing a
fault-tolerant service by replicating servers and coordinating the execution of
client commands against server replicas . State machine
replication ensures strong consistency (i.e., linearizability )
by coordinating the execution of commands in the different replicas: Every
replica has a full copy of the service state  and
executes commands submitted by the clients in the same order. To guarantee that
servers deliver the same sequence of commands, SMR can be implemented with
atomic broadcast: commands are atomically broadcast to all servers, and all
correct servers deliver and execute the same sequence of commands
.


[Replication and Scalability]Replication and Scalability


Enterprises of all sizes are embracing rapid modernization of user-facing
applications as part of their broader digital transformation strategy. The
distributed infrastructure that such applications rely on suddenly needs to
support much larger data sizes and transaction volumes. Scalability, which is
the ability to increase performance of a system by upgrading existing resources
(scale up) or aggregating additional resources to the existing ones (scale out),
is fundamental to meeting the performance requirements of modern services.

Partitioning (or sharding in database context) is a technique to make different
sets of machines do different work in order to achieve scalability. In
distributed systems, State Machine Replication is used to provide
fault-tolerance by running the same code and database on replicated machines.
But Replication only improves fault-tolerance on the cost of more machines.
Partitioning allows us to distribute different jobs/data to different sets of
machines. Most trivially, it can be done by allocating different key ranges to
different replicated groups in a Key/Value data store.

Scalability in distributed database system

Sharding is a database architecture pattern related to horizontal partitioning —
the practice of separating one table’s rows into multiple different tables,
known as partitions. Each partition has the same schema and columns, but also
entirely different rows. Likewise, the data held in each is unique and
independent of the data held in other partitions. Sharding involves breaking up
one’s data into two or more smaller chunks, called logical shards. The logical
shards are then distributed across separate database nodes, referred to as
physical shards, which can hold multiple logical shards. Despite this, the data
held within all the shards collectively represent an entire logical dataset.

Google Spanner  was designed to be a highly-scalable
distributed relational database. Spanner consists of multiple zones, each
of which is a deployment of Bigtable servers. A zone uses one zonemaster
to assign data to one hundred to several thousand sets of Paxos-based state
machines (so called spanservers).


The steps to process a multi-partition transaction are the following. First, a
client communicates with a proxy location to determine which groups maintain the
data touched by the transaction. Second, the client retrieves this data from the
groups, acquiring locks for them. Next, the client executes its transaction
locally, chooses one of the groups involved in the transaction as a coordinator
group C, and sends the result and the id of C to all groups involved in the
transaction. Finally, group C coordinates a two-phase commit with the other
groups for committing the transaction

Although not detailed in the paper, Spanner allows data to be re-sharded across
spanservers or zones data centers to balance loads and in response
to failures by placement driver. Periodically, placement driver
communicates with spanservers to re-arrange data. During these transfers,
clients can still execute transactions (including updates) on the database.
Figure  shows an execution of a single transactions that
requires multiple round of coordination in Spanner.

Distributed operators appear in execution plans with a distributed union variant
on top of one or more local distributed union variants. A distributed union
variant performs the remote distribution of subplans. A local distributed union
variant is on top of each of the scans performed for the query, as shown in the
execution plan in Figure . The local distributed union
variants ensure stable query execution when restarts occur for dynamically
changing split boundaries.Whenever possible, a distributed union variant has a
split predicate that results in split pruning, meaning the remote servers
execute subplans on only the splits that satisfy the predicate. This improves
both latency and overall query performance.









*








*


Scaling state machine replication

The classical SMR does not scale: adding resources (e.g., replicas) will not
translate into the improvement of performance, in terms of throughput. This
happens for a couple reasons. First, the underlying communication protocol
needed to ensure ordered message delivery may not scale itself (i.e., a
communication bottleneck). Second, every replica must execute the same sequence
of command to reach the same state (i.e., an execution bottleneck).

Several approaches have been proposed to address SMR's scalability limitations.
To cope with communication overhead, some proposals have suggested to spread the
load of ordering commands among multiple processes (e.g.,
), as opposed to dedicating a single
process to determine the order of commands (e.g.,
).

Two directions of research have been suggested to overcome execution
bottlenecks. One approach (scaling up) is to take advantage of multiple cores to
execute commands concurrently without sacrificing consistency
. Another approach
(scaling out) is to partition the service's state and replicate each partition
(e.g., ). In this chapter, we review the
second category, which is be basic of our work.

Partitioning application state

Modern distributed systems typically scale performance by partitioning
application state and tolerate failures by replicating each partition. Clients
submit commands for execution to one or more partitions. Within a partition,
replicas coordinate by means of a consensus protocol (e.g., Paxos ).
To coordinate the execution of multi-partition commands, replicated partitions
rely on some distributed coordination protocol (e.g., two-phase locking
, optimistic concurrency control ,
atomic multicast ).

In principle, increasing the number of partitions should result in increased
system performance. However, if the execution of commands involves multiple
partitions, then performance can actually decrease, due to overhead from
ordering and coordinating commands across partitions to ensure strong
consistency. Different techniques have been proposed to handle commands that
access state in multiple partitions, but inherently multi-partition commands are
more expensive than single-partition commands. Moreover, if data is not
distributed carefully, then load imbalances can nullify the benefits of
partitioning. Thus, an ideal partitioning scheme is one that would both (i)
allow commands to be executed at a single partition only, and (ii) evenly
distribute data so that load is balanced among partitions. We refer to workloads
that can be partitioned in a way that satisfies these two properties as
exhibiting strong locality. Conversely, workloads that cannot avoid
multi-partition commands with balanced load among partitions exhibit weak
locality.

Broadly speaking, there are two classes of techniques for partitioning:
static and dynamic. Figure  shows the result
of a motivating experiment that compares two representative systems, one of each
class. In the experiment, we measured the throughput and number of state moves
over time with two different workloads: one with strong locality and one with
weak locality. The workloads are inspired by the social network Twitter, in
which the network is modeled as a graph, and users can ``post'' messages. The
social graph was generated using a Zipfian distribution, where the Zipf
parameter was adjusted to alter the locality.  For brevity, we postpone the
details of the experimental setup until Section .
Static techniques choose an immutable assignment of application state
variables to partitions prior to executing commands. This techniques requires a
good understanding about the workload to avoids load imbalances and favors
single-partition commands.  Moreover, many online applications experience
variations in demand. These happen for a number of reasons. In social networks,
for example, some users may experience a surge increase in their number of
followers (e.g., new ``celebrities"); workload demand may shift along the hours
of the day and the days of the week; and unexpected (e.g., a video that goes
viral) or planned events (e.g., a new company starts trading in the stock
exchange) may lead to exceptional periods when requests increase significantly
higher than in normal periods. These challenges perhaps explain why most
approaches that extend SMR with state partitioning delegate the task of
partitioning the service state to the application designer. Dynamic
techniques address the limitations of static techniques by adapting the
partitioning scheme as workloads change. For example, a dynamic technique can
move data ``on demand'' to maximize the number of single partition user
commands, while avoiding imbalances in the load of the partitions. The major
challenge in designing a dynamic scheme is determining how the system selects
the partition to which to move data.


























*

Static state partitioning with S-SMR


In this section, we describe  an extension to SMR that under certain
workloads allows performance to grow proportionally to the number of replicas.
 partitions the service state and replicates each partition. It relies on
an atomic multicast primitive to consistently order commands within and across
partitions. In addition,  assumes a static workload partitioning. Any
state reorganization requires system shutdown and manual intervention.

In S-SMR , the service state  is composed of 
partitions, in set . Server group
 is assigned to partition . For brevity, we say that
server  belongs to  meaning that , and say
``multicast to '' meaning ``multicast to server group ''.
S-SMR relies on an oracle, which tells which partitions are accessed by
each command.














*

To execute a command, the client multicasts the command to the appropriate
partitions, as determined by the oracle. Commands that access a single partition
are executed as in classical SMR: replicas of the concerned partition agree on
the execution order and each replica executes the command independently. In the
case of a multi-partition command, replicas of the involved partitions deliver
the command and then may need to exchange state in order to execute the command
since some partitions may not have all the values read in the command. This
mechanism allows commands to execute seamlessly despite the partitioned state.

S-SMR improves on classical SMR by allowing replicated systems to scale, while
ensuring linearizability. Under workloads with multi-partition commands,
however, it has limited performance, in terms of latency and throughput
scalability. Such decreased performance when executing multi-partition commands
is due to partitions (i) exchanging state variables and (ii) synchronizing by
exchanging signals. Thus, the performance of  is particularly
sensitive to the way the service state is partitioned.

One way to reduce the number of multi-partition commands is by dynamically
changing the partitioning, putting variables that are usually accessed together
in the same partition. However, the partitioning oracle of  relies on a
static mapping of variables to partitions. One advantage of this implementation
is that all clients and servers can have their own local oracle, which always
returns a correct set of partitions for every query. Such a static mapping has
the major limitation of not allowing the service to dynamically adapt to
different access patterns.

Dynamic state partitioning








































































[Dynamic partitioning for SMR]Dynamic partitioning for SMR




An inherent problem of traditional SMR is that it is not scalable: any replica
added to the system will deliver all requests, so throughput is not increased.
Scalable SMR addresses this issue in two ways: (i) by partitioning the
application state, while allowing every command to access (read/write) any
combination of partitions and (ii) using caching to reduce the communication
across partitions, while keeping the execution linearizable.

On the downside of this approach, as the number of multi-partition commands
increases, performance of  becomes worse, as partitions must communicate.
One way to reduce the number of multi-partition commands is by dynamically
changing the partitioning, putting variables that are usually accessed together
in the same partition. However, the partitioning oracle of  relies on a
static mapping of variables to partitions. One advantage of this implementation
is that all clients and servers can have their own local oracle, which always
returns a correct set of partitions for every query. Such a static mapping has
the major limitation of not allowing the service to dynamically adapt to
different access patterns. Any state reorganization requires system shutdown and
manual intervention.

Given these issues, it is crucial that highly available partitioned systems be
able to dynamically adapt to the workload. In this chapter, we present
 (), a technique that allows a partitioned SMR system to
reconfigure its data placement on-the-fly.  achieves dynamic data
reconfiguration without sacrificing scalability or violating the properties of
classical SMR. These requirements introduce significant challenges. Since state
variables may change location, clients must find the current location of
variables. The scalability requirement rules out the use of a centralized oracle
that clients can consult to find out the partitions a command must be multicast
to. Even if clients can determine the current location of the variables needed
to execute a command, by the time the command is delivered at the involved
partitions one or more variables may have changed their location. Although the
client can retry the command with the new locations, how to guarantee that the
command will succeed in the second attempt? In classical SMR, every command
invoked by a non-faulty client always succeeds.  should provide similar
guarantees.

 was designed to exploit workload locality. Our scheme benefits from
simple manifestations of locality, such as commands that repeatedly access the
same state variables, and more complex manifestations, such as structural
locality in social network applications, where users with common interests have
a higher probability of being interconnected in the social graph. Focusing on
locality allows us to adopt a simple but effective approach to state
reconfiguration: whenever a command requires data from multiple partitions, the
variables involved are moved to a single partition and the command is executed
against this partition. To reduce the chances of skewed load among partitions,
the destination partition is chosen randomly. Although  could use more
sophisticated forms of partitioning, formulated as an optimization problem
(e.g., ), our technique has the advantage that
it does not need any prior information about the workload and is not
computationally expensive.

To track object locations without compromising scalability, in addition to a
centralized oracle that contains accurate information about the location of
state variables, each client caches previous consults to the oracle. As a
result, the oracle is only contacted the first time a client accesses a variable
or after a variable changes its partition. Under the assumption of locality, we
expect that most queries to the oracle will be accurately resolved by the
client's cache. To ensure that commands always succeed, despite concurrent
relocations, after attempting to execute a command a few times unsuccessfully,
 retries the command using 's execution atomicity and involving
all partitions. Doing so increases the cost to execute the command but
guarantees that relocations will not interfere with the execution of the
command.

We have fully implemented  as the  Java library, and we
performed a number of experiments using , a social network
application built with . We compared the performance of 
to  using different workloads. With a mixed workload that combines various
operations issued in a social network application,  reached 74 kcps
(thousands of commands per second), against less than 33 kcps achieved by
, improving by a factor of over 2.2. Moreover, performance
scales with the number of partitions under all workloads.

The following contributions are presented in this chapter:
(1) It introduces  and discusses some performance optimizations, including
the caching technique.
(2) It details , a Java library to simplify the design of services based
on .
(3) It describes  to demonstrate how  can be used to implement
a scalable social network service.
(4) It presents a detailed experimental evaluation of , deploying it with
 and  in order to compare the performance of the two replication
techniques.

The remainder of this chapter is organized as follows.
Section  gives an overview of 
Section  explains the algorithm in detail.
Section  proposes some performance optimizations.
Section  argues about the correctness of the algorithm.
Section  details the implementation of 
Section  reports on the performance of and 
Section  concludes the chapter.


General idea


Dynamic  () defines a dynamic mapping of variables to partitions.
Each variable  is mapped to partition , meaning that . Such
a mapping is managed by a partitioning oracle, which is implemented as a
replicated service run by a group of server processes . The oracle
service allows the mapping of variables to partitions to be retrieved or changed
during execution. In more detail,  distinguishes five types of commands:
 is an application command that accesses (reads or writes)
variables in set  (as described in
Section ),  creates a new variable  and
initially maps it to a partition defined by the oracle,  removes 
from the service state,
 moves variable  from partition  to partition
, and  asks the oracle which variables are accessed by
command , and which partition contains each of them. The reply from the
oracle to a  command is called a . A prophecy usually
consists of a set of tuples , meaning that variable 
is mapped to partition . The other possible values for a prophecy are 
and , which mean that a command can and cannot be executed, respectively.

Clients can consult the oracle to know which partitions each command should be
multicast to, based on which variables are accessed by the command. If the reply
received from the oracle tells the client that the command accesses a single
partition, the client multicasts the command to that partition. If the command
accesses variables from multiple partitions, the client first multicasts one or
more  commands to the oracle and to the involved partitions, with the
intent of having all variables in the same partition. Then, the command itself
is multicast to the one partition that now holds all variables accessed by the
command. If a subsequent command accesses the same variables, it will also
access a single partition. With this scheme, the access patterns of commands
will shape the mapping of variables to partitions, reducing the number of
multi-partition commands.








*

Consulting the oracle and issuing the application command are done with separate
calls to atomic multicast in . It may happen that, between those
operations, the partitioning changes. We illustrate this in
Figure . Commands  and  read variable . Since
partitioning is dynamic, the client issuing the commands first consults the
oracle before multicasting each command.  executes without the interference
of other commands, so consulting the oracle and multicasting the command only
once is enough for  to be executed. However, before  is multicast to
, another client issues a  command that relocates  to .
When  is delivered at the servers of , the command is not executed,
since  is not available at  anymore. A similar situation may arise
when a command accesses variables from multiple partitions, as it consists of
multicasting at least three commands separately: ,  and .
The partitioning can change between the execution of any two of those commands.

To solve this problem, the client multicasts the set of variables accessed along
with each access command. Upon delivery, each server checks the set of variables
sent by the client. If all variables in the set belong to the local partition,
the command is executed; otherwise, a  message is sent back to the
client. When the client receives a  message, it consults the oracle
again, possibly moving variables across partitions, and then reissues the access
command. To guarantee termination, if the command fails a certain number of
times, the client multicasts the command to all partitions and the servers
execute it as in the original .

The  client consists of the application logic and a client proxy.
The application does not see the state variables divided into partitions. When
the application issues a command, it sends the command to the proxy and
eventually receives a reply. All commands that deal with partitioning (i.e.,
consulting the oracle, moving objects across partitions and retrying commands as
described in the previous paragraph) are executed by the client proxy,
transparently to the application. When the client proxy multicasts a
partitioning-related command to multiple partitions and the oracle, partitions
and oracle exchange signals to ensure linearizability, as mentioned in
Section . Every server and oracle process has its own 
proxy as well. At each server, the proxy checks whether commands can be executed
and manages the exchange of data and signals between processes. At the oracle,
the service designer defines the application-dependent rules that must be
followed (e.g., where each variable is created at first) and a proxy is
responsible for managing the communication of the oracle with both clients and
servers when executing commands.  relies on a fault-tolerant multicast
layer for disseminating commands across replicas and implementing reliable
communication between partitions. Replies to commands are sent directly through
the network. Figure  illustrates the architecture of .








*


Detailed algorithm



When issuing a command, the application simply forwards the command to the
client proxy and waits for the reply. Consulting the oracle and multicasting the
command to different partitions is done internally by the proxy at the client.
Algorithms , , and
 describe in detail how the  proxy works
respectively at client, server and oracle processes. Every server proxy at a
server in  has only partial knowledge of the partitioning: it knows only
which variables belong to . The oracle proxy has knowledge of every
. To maintain such a global knowledge, the oracle must 
every command that creates, moves, or deletes variables. (In
Section , we introduce a caching mechanism to prevent the oracle
from becoming a performance bottleneck.)

[t!]


[1]



To issue a command , the client proxy does:



    do
        oracle, 
        wait for 
        
            
        
            
             is an  command and 
                let  be one of the partitions in 
                
                each 
                    // move  to partition 
                    let  be 
                    
                        
                        oracle    
                        , 
                    
                
                
            
             is  or 
                
            
            , 
            wait for 
        
    
    while  // after many retries, fall back to 
    return  to the application client


 Client Proxy




The client proxy. To execute a command , the proxy first consults
the oracle. The oracle knows all state variables and which partition contains
each of them. Because of this, the oracle may already tell the client whether
the command can be executed or not. Such is the case of the 
command: if there is a variable  that the command tries to read or
write and  does not exist, the oracle already tells the client that the
command cannot be executed, by sending  as the prophecy. A  prophecy
is also returned for a  command when  already exists. For a
 command when  already does not exist, an  prophecy is
returned. If the command can be executed, the client proxy receives a prophecy
containing a pair , for every variable  created,
accessed or deleted by the command. If the prophecy regarding an
 command contains multiple partitions, the client proxy chooses
one of them, , and tries to move all variables in  to .
Then, the command  itself is multicast to . As discussed in
Section , there is no guarantee that an interleave of
commands will not happen, even if the client waits for the replies to the move
commands. For this reason, and to save time, the client proxy multicasts all
move commands at once. Commands that change the partitioning (i.e., create and
delete) are also multicast to the oracle. If the reply received to the command
is , the procedure restarts: the proxy consults the oracle again,
possibly moves variables across partitions, and multicasts  to the
appropriate partitions once more. After reaching a given threshold of retries
for , the proxy falls back to , multicasting  to all partitions
(and the oracle, in case  is a create or delete command), which ensures the
command's termination.

[t!]


[1]



To execute a command , the server proxy in partition  does:

    
    
    when 
        
    

    

    when 

    

         is an  command
            
                reply with 
            
                have the command executed by the application server
                send the reply to the client
            
        

        
    
         is a  command
            
                
                    ,
                    
                
                    ,
                
            
                wait until 
                
                    
                    
                
            
        
        
    
         is a  command
            wait until 
            
                



            
        
        
        
         is a  command
            
                



            
        
    


 Server Proxy




The server proxy. Upon delivery, access commands are intercepted by the
 proxy before they are executed by the application server. In ,
every access command is executed in a single partition. If a server proxy in
partition  intercepts an  command that accesses a variable
 that does not belong to , it means that the variable is in
some other partition, or it does not exist. Either way, the client should retry
with a different set of partitions, if the variable does exist. To execute a
 command, the server proxy at partition  simply removes 
from partition , in case . In case , it might
be that the variable exists but belongs to some other partition . Since
only the oracle and the servers at  have this knowledge, it is the oracle
who replies to delete commands.

 server and oracle proxies coordinate to execute commands that create or
move variables. Such coordination is done by means of . When a
 command is delivered at , the server proxy waits for a message
from the oracle, telling whether the variable can be created or not, to be
ed. Such a message from the oracle is necessary because  might not
belong to , but it might belong to some other partition  that
servers of  have no knowledge of. If the create command can be executed,
the oracle can already reply to the client with a positive acknowledgement,
saving time. This can be done because atomic multicast guarantees that all
non-faulty servers at  will eventually deliver and execute the command. As
for move commands, each  command consists of moving
variable  from a source partition  to a destination partition
. If the server's partition  is the source partition (i.e., 
= ), the server proxy checks whether  belongs to . If 
, the proxy s  to , so that servers
at the destination partition know the most recent value of ;  is sent
along with  to inform which move command that message is related to. If 
, a  message is  to ,
informing  that the move command cannot be executed.

[t!]


[1]



To execute a command , the oracle proxy does:

    

    when 

    

         is a  command
            

             is an  command
                
                    
                
                    each 
                        
                    
                

             is a  command
                
                    
                
                     initial partition, defined by application rules
                    
                

             is a  command
                
                    
                
                    
                
                
            
            
            send  to the client

        

         is a  command
            
                
                
            

        
    
         is a  command
            let  be oracle
            
                
            
                
            
            
            send  to the client
        
        
        
         is a  command
            let  be oracle
             or 
                send     to the client
            
                send  to the client
            
            
            
        
    


 Oracle Proxy




The oracle proxy. One of the purposes of the oracle proxy is to make
prophecies regarding the location of state variables. Such prophecies are used
by client proxies to multicast commands to the right partitions. A prophecy
regarding an  command contains, for each , a pair
, meaning that . If any of the variables in
 does not exist, the prophecy already tells the client that the command
cannot be executed (with a  value). For a  command, the prophecy
tells where  should be created, based on rules defined by the application, if
 does not exist. If  already exists, the prophecy will contain , so
that the client knows that the create command cannot be executed. The prophecy
regarding a  command contains the partition that contains , or
, in case  was already deleted or never existed.

Besides dispensing prophecies, the oracle is responsible for executing create,
move, and delete commands, coordinating with server proxies when necessary, and
replying directly to clients in some cases. For each 
command, the oracle checks whether  in fact belongs to the source partition
. If that is the case, the command is executed, moving  to .
Each  command is multicast to the oracle and to a partition .
If  already exists, the oracle tells  that the command cannot be
executed, by ing  to . The oracle also sends  to the
client as reply, meaning that  already exists. If  does not exist, the
oracle tells  that the command can be executed, by ing  to
. It also tells the client that the command succeeded with an  reply.
Finally, each  command is multicast to the oracle and to a partition
, where the client proxy assumed  to be located. If  belongs to
, or  does not exist, the oracle tells the client that the delete
command succeeded. Otherwise, that is, if  exists, but  was
multicast to the wrong partition, the oracle tells the client to retry.



Performance optimizations


In this section, we introduce two optimizations for : caching and load
balancing.

Caching. In Algorithm , for every command issued
by the client, the proxy consults the oracle. If every command passes by the
oracle, the system is unlikely to scale, as the oracle is prone to becoming a
bottleneck. To provide a scalable solution, each client proxy has a local cache
of the partitioning information. Before multicasting an application command 
to be executed, the client proxy checks whether the cache has information about
every variable concerned by . If the cache does have such a knowledge, the
oracle is not consulted and the information contained in the cache is used
instead. If the reply to  is , the oracle is consulted and the
returned prophecy is used to update the client proxy's cache.
Algorithm  is followed from the second attempt to execute
 on. The cache is a local service that follows an algorithm similar to that
of the oracle, except it responds only to  commands and, in
situations where the oracle would return  or , the cache tells the
client proxy to consult the actual oracle.


Naturally, the cached partitioning information held by the client proxy may be
out of date. On the one hand, this may lead a command to be multicast to the
wrong set of partitions, which will probably incur in the client proxy having to
retry executing the command. For instance, in Figure  the
client has an out-of-date cache, incurring in a new consultation to the oracle
when executing . On the other hand, the client proxy may already have to
retry commands, even if the oracle is always consulted first, as shown in
Figure . If most commands are executed without consulting
the oracle, as in the case of  in Figure , we avoid
turning the oracle into a bottleneck. Moreover, such a cache can be updated
ahead of time, not having to wait for an actual application command to be issued
to only then consult the oracle. This way, the client proxy can keep a cache of
partitioning information of variables that the proxy deems likely to be accessed
in the future.

Load balancing. When moving variables, the client proxies may try to
distribute them in a way that balances the workload among partitions. This way,
the system is more likely to scale throughput with the number of server groups.
One way of balancing load is by having roughly the same number of state
variables in every partition. This can be implemented by having client proxies
choosing randomly the partition that will receive all variables concerned by
each command (at line  of
Algorithm ). Besides improving performance, balancing the
load among partitions prevents the system from degenerating into a
single-partition system, with all variables being moved to the same place as
commands are executed.










*


Correctness


In this section, we argue that  ensures termination and linearizability.
By ensuring termination, we mean that for every command  issued by a correct
client, a reply to  different than  is eventually received by the
client. This assumes that at least one oracle process is correct and that every
partition has at least one correct server. Given these constraints, the only
thing that could prevent a command from terminating would be an execution that
forced the client proxy to keep retrying a command. This problem is trivially
solved by falling back to  after a predefined number of retries: at a
certain point, the client proxy multicast the command to all server and oracle
processes, which execute the command as in , i.e., with coordination
among all partitions and the oracle.

As for linearizability, we argue that, if every command in execution  of
 is delivered by atomic multicast and is execution atomic (as
defined in ), then  is linearizable. We denote the
order given by atomic multicast by relation . Given any two messages
 and , ``'' means that there exists a process that
delivers both messages and  is delivered before , or there is some
message  such that  and , which can be written
as .

Also, for the purposes of this proof, we consider the oracle to be a partition,
as it also s and executes application commands.

Suppose, by means of contradiction, that there exist two commands  and ,
where  finishes before  starts, but  in the execution. There
are two possibilities to be considered: (i)  and  are delivered by the
same process , or (ii) no process delivers both  and .

In case (i), at least one process  delivers both  and . As  finishes
before  starts, then  delivers , then . From the properties of
atomic multicast, and since each partition is mapped to a multicast group, no
process delivers , then . Therefore, we reach a contradiction in this
case.

In case (ii), if there were no other commands in , then the execution of 
and  could be done in any order, which would contradict the supposition that
. Therefore, there are commands  with atomic order 
, where some process  (of
partition ) delivers , then ; some process 
delivers , then , and so on: process  delivers
, then , where . Finally, process 
delivers , then .

Let  and let  be the following predicate: ``For every
process ,  finishes executing  only after some 
 started executing .'' We now claim that  is true for
every , where . We prove our claim by induction.




Basis ():  is obviously true, as  can only finish
executing  after starting executing it.


Induction step: If , then . 

Proof: Command  is multicast to both  and . Since
 is execution atomic, before any  finishes
executing , some  starts executing . Since
, every  start executing  only after
finishing the execution of . As  is true, this will only happen
after some  started executing .



As , for every ,  executes command  only
after the execution of  at  finishes. From the above claim, this
happens only after some  starts executing . This means that
 () was issued by a client before any client received a response for
, which contradicts the assumption that  precedes  in real-time, i.e.,
that command  was issued after the reply for command  was received.


Implementation


In this section, we describe , a library that implements both 
and , and , a scalable social network application built with
.  and  were both implemented in Java.



To implement a replicated service with , the developer (i.e., service
designer) must extend three classes: PRObject, StateMachine, OracleStateMachine.

The PRObject class.  supports partial replication (i.e., some
objects may be replicated in some partitions, not all). Therefore, when
executing a command, a replica might not have local access to some of the
objects involved in the execution of the command. The developer informs
 which object classes are partially replicated by extending the
PRObject class. Each object of such class is stored either locally or remotely,
but the application code is agnostic to that. All calls to methods of such
objects are intercepted by , transparently to the developer.




The StateMachine class. This class implements the logic of the server
proxy. The application server class must extend the StateMachine class. To
execute commands, the developer must provide an implementation for the method
executeCommand(Command). The code for such a method is agnostic to the existence
of partitions. In other words, it can be exactly the same as the code used to
execute commands with classical state-machine replication (i.e., full
replication).  is responsible for handling all communication between
partitions and oracle transparently. To start the server, method
runStateMachine() is called. Method createObject() also needs to be implemented,
where the developer defines how new state objects are loaded or created.

The OracleStateMachine class. This class implements the logic of the
oracle proxy. It extends StateMachine, so the oracle can be deployed similarly
to a fault-tolerant partition in the original . Class OracleStateMachine
has a default implementation, but the developer is encouraged to override its
methods. Method extractObject(Command) returns the set of objects accessed by
the command. It should be overridden by the application so that the client proxy
can relocate all necessary objects to a destination partition before executing
the application command.

Method getTargetPartition(SetObject) returns a particular
partition to which objects should be moved, when they are not in the same
partition yet, in order to execute an application command that accesses those
objects. The default implementation of the method returns a random partition.
The developer can override it in order to further improve the distribution of
objects among partitions. For instance, the destination partition could be
chosen based on an attribute of the objects passed to the method.

The client proxy is implemented in class Client, which handles all communication
of the application client with the partitioned service. The client proxy
provides methods sendCreate(Command, CallbackHandler), sendAccess(Command,
CallbackHandler), and sendDelete(Command, CallbackHandler). The client proxy's
default behavior is to keep retrying commands (and fallback to  in case of
too many retries) and only call back the application client when the command has
been successfully executed. However, the developer can change this behavior by
overriding the error() method of CallbackHandler. The error() method is called
when a  reply is received.






We implemented , a social network application similar to Twitter,
using . Twitter is an online social networking service in which users
can post 140-character messages and read posted messages of other users. The API
consists basically of: post (user publishes a message), follow (user starts
following another user), unfollow (user stops following someone), and
getTimeline (user requests messages of all people whom the user follows).

State partitioning in  is based on users' interest. A function 
returns the partition that user with id  should belong to, based on the
user's interest. Function  is implemented in method getObjectPlacement(User)
of class Oracle, which extends OracleStateMachine (class User extends
PRObject). Taking into account that a typical user probably spends more time
reading messages (i.e., issuing getTimeline) than writing them (i.e., issuing
post), we decided to optimize getTimeline to be single-partition. This means
that, when a user requests his or her timeline, all messages should be available
in the partition that stores that user's data, in the form of a materialized
timeline (similarly to a materialized view in a database). To make this
possible, whenever a post request is executed, the message is inserted into the
materialized timeline of all users that follow the one that is posting. Also,
when a user starts following another user, the messages of the followed user are
inserted into the follower's materialized timeline as part of the command
execution; likewise, they are removed when a user stops following another user.
Because of this design decision, every getTimeline request accesses only one
partition, follow and unfollow requests access objects on at most two
partitions, and post requests access up to all partitions. The  client
does not need any knowledge about partitions, since it uses method
sendAccessCommand(command) of the  client proxy to issue its commands.

One detail about the post request is that it needs access to all users that
follow the user issuing the post.




To ensure linearizability when executing a post request, the  server
overrides the extractObject(command) method to check if all followers that will
be accessed by the command are available in the local partition (i.e., the
partition of the server executing the post command). If this is the case, the
request is executed. Otherwise, the server sends a  message,
where  is the complete set of followers of the user who was posting.
Then, the  server proceeds to the next command. Upon receiving the
 message, the client proxy tries to move all users in  to
the same partition before retrying to execute the post command.


Performance evaluation


In this section, we present the results found for  with different loads
and partitionings and compare them with the original
 .



In Section , we describe the environment where we
conducted our experiments. In Section , we show
the results.











*

Environment setup and configuration parameters


We conducted all experiments on a cluster that had two types of nodes: (a) HP
SE1102 nodes, equipped with two Intel Xeon L5420 processors running at 2.5 GHz
and with 8 GB of main memory, and (b) Dell SC1435 nodes, equipped with two AMD
Opteron 2212 processors running at 2.0 GHz and with 4 GB of main memory. The HP
nodes were connected to an HP ProCurve 2920-48G gigabit network switch, and the
Dell nodes were connected to another, identical switch. Those switches were
interconnected by a 20 Gbps link. All nodes ran CentOS Linux 7.1 with kernel
3.10 and had the OpenJDK Runtime Environment 8 with the 64-Bit Server VM
(build 25.45-b02).



For the experiments, we use the following workloads: Timeline (composed only of
getTimeline requests), Post (only post requests), Follow/unfollow (50 of
follow requests and 50 of unfollow), and Mix (7.5 post, 3.75 follow,
3.75 unfollow, and 85 getTimeline).

Results 




We can see in Figure 

the results achieved with .

For the Timeline workload, the throughput with  and  are very
similar. This happens because getTimeline requests are optimized to be
single-partition: all posts in a user's timeline are stored along with the User
object.


This is the ideal workload for . In , the partitioning does not
change, and consulting the oracle becomes unnecessary thanks to the local cache
at each client. This happens because there are no other commands in the Timeline
workload.

In the Post workload, every command accesses up to all partitions in the system,
which is the worst case for : the more partitions are involved in the
execution of a command, the worst is the system's performance. We can see that
the throughput of  decreases significantly as the number of partitions
increases. For , we can see that the system throughput scales with the
number of partitions. This happens because User objects that are accessed
together, but which are in different partitions, are moved to the same partition
based on the interests of the users. As the execution proceeds, this leads to a
lower rate of multi-partition commands, which allows throughput to scale. (In
the case of posts on 2 partitions, the number of move commands started at 3
kcps, with throughput of 23 kps, and eventually reduced to less than 0.1 kcps.)
As a result the throughput improvement of  with respect to 
increases over time. With eight partitions,  sports a performance that
is 45 times that of !

With the Follow/unfollow workload, the system performs in a similar way to that
observed with the Post workload. The difference is that each follow or unfollow
request accesses only two User objects, whereas every post request may affect an
unbounded number of users. For this reason, each follow/unfollow command is
executed at most by two partitions in . In , a single move
command is enough to have all User objects affected by such a command in the
same partition. For this reason, both replication techniques have better
throughput under the Follow/unfollow workload than with Post. As with the Post
workload, 's advantage over  increases with the number of
partitions, reaching up to almost 25 times with eight partitions.

We approximate a realistic distribution of commands with the Mix workload. With
such a workload,  does not perform as bad as in the Post or
Follow/unfollow workloads, but the system throughput still decreases as
partitions are added. As with the other workloads,  scaled under the Mix
workload. With eight partitions, it reached 74 kcps (thousands of commands per
second), fairly close to the ideal case (the Timeline workload), where 
reached 86 kcps. Under the Mix workload,  had less than 33 kcps in the
best case (one partition) and around 10 kcps with eight partitions. In the
configuration with eight partitions,  reaches almost seven times 
throughput.

Latency values with  are higher than with . This was expected for
two reasons. First, there is an extra group of servers (the oracle) to
communicate with. Second, executing a command often means moving all accessed
objects to the same partition. Taking this into account, we consider the (often
slight) increase in latency observed with  a low price to pay for the
significant increase in throughput and the scalability that  brought to
the system; with , the system did not scale with multi-partition
commands.








Conclusion

This chapter introduced , the initial idea of . So far, we have
proposed an approach that allows scaling state-machine replication by
dynamically repartitioning application state based on the workload.  Variables
that are usually accessed together are moved to the same partition, which
significantly improves scalability. However, in order to reduce the chances of
skewed load among partitions in  the destination partition is chosen
randomly. Although this heuristic algorithm could bring a balanced partitioning,
it fails to guarantee optimal partitioning and to minimize the rate of
multi-partition commands. In the next chapter, we will discuss more details
about reaching an optimized partitioning.


[Dynastar: Optimized partitioning for SMR]Dynastar: Optimized partitioning for SMR



 addresses the limitations of  by adapting the partitioning
scheme as workloads change, by moving data ``on demand'' to maximize the number
of single partition user commands, while avoiding imbalances in the load of the
partitions. The major challenge in this approach is determining how the system
selects the partition to which to move data.  selects partitions
randomly, which allows for a completely decentralized implementation, in which
partitions make only local choices about data movement. We refer to this
approach as decentralized partitioning. This approach works well for data
with strong locality, but it is unstable for workloads with weak
locality 



.
This happens because with weak locality, objects in  are constantly
being moved back and forth between partitions without converging to a stable
configuration.

In this chapter, we introduce , a new dynamic approach to the state
partitioning problem. Like the other dynamic approach, does not
require any a priori knowledge about the workload. However, differs
from the prior approach because it maintains a location oracle with a global
view of the application state. The oracle minimizes the number of state
relocations by monitoring the workload, and re-computing an optimized
partitioning on demand using a static partitioning algorithm.

The location oracle maintains two data structures: (i) a mapping of application
state variables to partitions, and (ii) a workload graph with state
variables as vertices and edges as commands that access the variables. Before a
client submits a command, it contacts the location oracle to discover the
partitions on which state variables are stored.  If the command accesses
variables in multiple partitions, the oracle chooses one partition to execute
the command and instructs the other involved partitions to temporarily relocate
the needed variables to the chosen partition. Of course, when relocating a
variable, the oracle is faced with a choice of which partition to use as a
destination. chooses the partition for relocation by partitioning the
workload graph using the METIS  partitioner and selecting
the partition that would minimize the number of state relocations.


To tolerate failures, implements the oracle as a regular partition,
replicated in a set of nodes. To ensure that the oracle does not become a
performance bottleneck, clients cache location information. Therefore, clients
only query the oracle when they access a variable for the first time or when
cached entries become invalid (i.e., because a variable changed location).
copes with commands addressed to wrong partitions by telling clients
to retry a command.

We implemented and compared its performance to alternative schemes,
including an idealized approach that knows the workload ahead of time. Although
this scheme is not achievable in practice, it provides an interesting baseline
to compare against. performance rivals that of the idealized scheme,
while having no a priori knowledge of the workload.

We show that largely outperforms existing dynamic schemes under two
representative workloads, the TPC-C benchmark and a social network service. We
also show that the location oracle never becomes a bottleneck.


In summary, this chapter makes the following contributions:

It introduces and discusses its implementation.
It evaluates different partitioning schemes for state machine replication
under a variety of conditions.
It presents a detailed experimental evaluation of using the
TPC-C benchmark and a social network service populated with a real social
network graph that contains half a million users and 14 million edges.



The rest of the chapter is structured as follows.
Section  introduces and describes the technique in detail.
Section  overviews our prototype.
Section  reports on the results of our experiments.
Section  surveys related work and
Section  concludes the chapter.






defines a dynamic mapping of application variables to partitions.
Application programmers may also define other granularity of data when mapping
application state to partitions. For example, in our social network application
(sec:imp:), each user (together with the information associated
with the user) is mapped to a partition; in our TPC-C implementation
(sec:imp:tpcc), every district in a warehouse is mapped to a partition.
Such a mapping is managed by a partitioning oracle, which is handled as a
replicated partition. The oracle allows the mapping of variables to partitions
to be retrieved or changed during execution. To simplify the discussion, in
sec:dynastar-overview-- we initially assume
that every command involves the oracle. In sec:optm, we explain how
clients can use a cache to avoid the oracle in the execution of most commands.

Overview


Clients submit commands to the oracle and wait for the reply. 
supports three types of commands:  creates a new variable  and
initially maps it to a partition defined by the oracle;  is an
application command that reads and modifies variables in set 
; and  removes  from the service state. The reply from the
oracle is called a , and usually consists of a set of tuples 
, meaning , and a target partition  on which
the command will be executed. The  could also tell the clients if a
command cannot be executed (e.g., it accesses variables that do not exist). If
the command can be executed, the client waits for the reply from the target
partition.

If a command  accesses variables in  on a single partition, the
oracle multicasts  to that partition for execution. If the command accesses
variables on multiple partitions, the oracle multicasts a 
 command to the involved partitions to gather all variables in  to
the target partition . After having all required variables, the target
partition executes command , sends the reply to the client, and returns the
variables to their source.

The oracle also collects hints from clients and partitions to build up a
workload graph and monitors the changes in the graph.

In the workload graph, vertices represent state variables and edges dependencies
between variables. An edge connects two variables in the graph if a command
accesses both of them.


Periodically, the oracle computes a new optimized partitioning and sends the
partitioning plan to all partitions. Upon delivering the new partitioning, the
partitions exchange variables and update their state accordingly. 
relocates variables without blocking the execution of commands.


The detailed protocol


Algorithms , , and
 describe the client, oracle, and server processes,
respectively. We omit the delete command since the coordination involved in the
create and delete commands are analogous.




The client process

To execute a command , the client atomically multicasts  to the oracle
(Algorithm 1). The oracle replies with a prophecy, which may already tell the
client that  cannot be executed (e.g., it needs a variable that does not
exist, it tries to create a variable that already exists). If  can be
executed, the client receives a prophecy containing the partition where  will
be executed. The client then waits for the result of the execution of .










The oracle

When the oracle delivers a request, it distinguishes between two cases (Task 1
in Algorithm ).

If the command is to create a variable , and  does not already
exist, the oracle chooses a random partition for , multicasts the create
command to the partition and itself, and returns the partition to the client as
a prophecy (Figure ).
If the command reads and writes existing variables, the oracle first
checks that all such variables exist. If the variables exist and they are all in
a single partition, the oracle multicasts the command to that partition for
execution. If the variables are distributed in multiple partitions, the oracle
deterministically determines the destination partition, and atomically
multicasts a command to the involved partitions so that all variables are
gathered at the destination partition. The oracle chooses as the destination
partition the partition that contains most of the variables needed by the
command. (In case of a tie, one partition is chosen deterministically, among
those that contain most variables.) Once the destination partition has received
all variables needed by the command, it executes the command and returns the
variables to their source partition.






[h!]



[1]






	
	

	
		oracle, 
		
		wait for 
		[if receive  then...]
			
			...there's nothing to execute
		[in this case,  is ]
			
			wait for  from a server in 
			
		
	
	
	
	
	
	
	
	
	
	
	
	

	
return  to the application
















Client



[t!]



[1]



[Task 1]
	case  is a  command:
		[if  already exists...]
			
			...notify client
		[if  doesn't exist...]
			 choose 's partition
			
			
			prepare client's response
			
			

		
	

	case  is any command, but :
		
		variables accessed by 
		[if  exists:]
			
			tell the client
		[if all vars in  exist]
			
			get all partition involved
			
			 will excecute 
			
			
		
	
	send  to the client



[Task 2]


	: 
	exchange signal...
	wait until 
	...to coordinate
	





[Task 3]

	




[Task 4]
	update  with 
	
	
		 compute  from 
		
		
	




[Task 5]
	apply 




Algorithm variables:



: the set of all variable and their locations

: the partition configuration of 


Oracle







[h!]



[1]

[Task 1]
	
	variables accessed by 
	[Task 1a]
		execute command 
		send response to the client
	[Task 1b]
		
		get all involved partition			
		
		  will execute 
		
		  will send variables

		[ is the target partition:] 
			wait until 
			
			wait for needed variables
			execute command 
			send response to the client
			
			
			return the variables		
		[if  is not the target partition:]
			
			all needed variables in 
			
			send variables
			
			wait until 
		
	





[Task 2]
	: send  to 
	exchange signal
	wait until 
	...coordinate
	
	send  to the client



[Task 3]
	for each  do 
	
		 is 
			
			 
 
				apply 
			
		
			
			
			: send to 
			send objs in 
		
	



[Task 4]
    




Algorithm variables:



: the partition configuration from the 








Server in partition 



[h!]
  
  
  
  [1]  
  
     deterministicly compute partition to execute
  command  from 
    return 
  
  
  
  
  
    
    get all involved partitions
    return 
  
  
  
  
  
  
  
  
  
  
  Shared functions of the oracle and servers 
  
  
  









*

Upon delivering a create (Task 2), the oracle updates its partition information.
As part of a create command, the oracle coordinates with the partition to ensure
correctness (Task 3) .

The oracle also keeps track of the workload graph by receiving hints with
variables (i.e., vertices in the graph) and executing commands (i.e., edges in
the graph). These hints can be submitted by the clients or by the partitions,
which collect data upon executing commands and periodically inform the oracle
(Task 4). The oracle computes a partitioning plan of the graph and multicasts it
to all servers and to itself. Upon delivering new partition plan, the oracle
updates its location map accordingly (Task 5).






To compute an optimized partitioning, the oracle uses a graph partitioner. A new
partitioning can be requested by the application, by a partition, or by the
oracle itself (e.g., upon delivering a certain number of hints). To determine
the destination partition of a set of variables, as part of a move, the oracle
uses

the last computed partitioning.

The server process

When a server delivers a command , it first checks if it has all variables
needed by . If the server has all such variables, it executes  and sends
the response back to the client (Tasks 1a and 2 in
Algorithm ). If not all the variables needed by  are in
that partition, the server runs a deterministic function to determine the
destination partition to execute  (Task 1b). The function uses as input the
variables needed by  and  itself. In this case, each server that is in the
multicast group of  but is not the destination partition sends all the needed
variables stored locally to the destination partition and waits to receive them
back. The destination partition waits for a message from other partitions. Once
all variables needed are available, the destination partition executes the ,
sends the response back to the client, and returns the variables to their
source. Periodically, the servers deliver a new partitioning plan from the
oracle (Task 3). Each server will send the variables to the designated
partition, as in the plan, and wait for variables from other partitions. Once a
server receives all variables, it updates its location map accordingly.







To determine the destination partition for a command, the servers uses the last
computed partitioning.


Performance optimization






In the algorithm presented in the previous section, clients always need to
involve the oracle, and the oracle dispatches every command to the partitions
for execution. Obviously, if every command involves the oracle, the system is
unlikely to scale, as the oracle will likely become a bottleneck. To address
this issue, clients are equipped with a location cache. Before submitting a
command to the oracle, the client checks its location cache. If the cache
contains the partition of the variables needed by the command, the client can
atomically multicast the command to the involved partition and thereby avoid
contacting the oracle.

The client still needs to contact the oracle in one of these situations:




(a) the cache contains outdated information; or (b) the command is a create, in
which case it must involve the oracle, as explained before.



If the cache contains outdated information, it may address a partition that does
not have the information of all the variables accessed by the command. In this
case, the addressed partition tells the client to retry the command. The client
then contacts the oracle and updates its cache with the oracle's response.
Although outdated cache information results in execution overhead, it is
expected to happen rarely since repartitioning is not frequent.













Correctness criterion


Our consistency criterion is linearizability.  A system is linearizable
if there is a way to reorder the client commands in a sequence that (i) respects
the semantics of the commands, as defined in their sequential specifications,
and (ii) respects the real-time precedence of commands .

Correctness proof
To prove that ensures linearizability, we must show that for any
execution  of the system, there is a total order  on client
commands that (i) respects the semantics of the commands, as defined in their
sequential specifications, and (ii) respects the real-time precedence of
commands (sec:dynastar-correctcrit).

Let  be a total order of operations in  that respects , the
order atomic multicast induces on commands.

To argue that  respects the semantics of  commands, let  be the -th
command in  and  a process in partition  that executes .


We claim that when  executes , it has updated values of variables in
, the variables accessed by . We prove the claim by induction on
. The base step trivially holds from the fact that variables are initialized
correctly. Let ,  be the last client command before 
in  that accesses , and  a process in  that executes .
From the inductive hypothesis,  has an updated value of  when it executes
. There are two cases to consider: (a) . In this case,  obviously
has an updated value of  when it executes  since no other command
accesses  between  and . (b) . Since processes in the
same partition execute the same commands, it must be that .
From the algorithm, when  executes ,  and when 
executes , . Thus,  executed a command to move  to
another partition after executing  and  executed a command to move 
to  before executing . Since there is no command that accesses 
between  and  in ,  has an updated  when it executes 
(from inductive hypothesis), and  receives the value of  at , it
follows that  has an updated  when it executes .







We now argue that there is a total order  that respects the real-time
precedence of commands in . Assume  ends before  starts, or
more precisely, the time  ends at a client is smaller than the time 
starts at a client, . Since the time  ends at the
server from which the client receives the response for  is smaller than the
time  ends at the client, , and the time 
starts at the client is smaller than the time  starts at the first server,
, we conclude that .

We must show that either ; or neither  nor .
For a contradiction, assume that  and let  be executed by
partition .

There are two cases:

[(a)]  is a client command executed by . In this case, since
 only starts after  at a server, it follows that 
, a contradiction.
[(b)]  is a client command executed by  that first involves a
move of variables  from  to . At , 
 since the move is only executed after 
ends. Since the move only finishes after variables in  are in  and
 can be executed, it must be that

. We conclude that 
, a contradiction.

Therefore, either  and from the definition of ,  precedes
 or neither  nor , and there is a total order in
which  precedes .




For termination, we argue that every correct client eventually receives a
response for every command  that it issues. This assumes that every partition
(including the oracle partition) is always operational, despite the failure of
some servers in the partition. For a contradiction, assume that some correct
client submits a command  that is not executed. Atomic multicast ensures that
 is delivered by the involved partition. Therefore,  is delivered at a
partition that does not contain all the variables needed by . As a
consequence, the client retries with the oracle, which moves all variables to a
single partition and requests the destination partition to execute , a
contradiction that concludes our argument.




Implementation


Atomic multicast

Our prototype uses the BaseCast atomic multicast protocol
, available as open
source.  Each group of
servers in BaseCast executes an instance of
Multi-Paxos. 
Groups coordinate to ensure that commands multicast to multiple groups are
consistently ordered (as defined by the atomic multicast properties
sec:amcast).



BaseCast is a genuine atomic multicast in that only the sender and destination
replicas of a multicast message communicate to order the multicast message.





Our  prototype is written as a Java 8 library.



Application designers who use to implement a replicated service must
 extend three key classes:
 
 [--] PRObject: provides a common interface for replicated data
 items.
 [--] ProxyClient: provides the communication between application
 client and server, while encapsulating all complex logic of handling caches or
 retried requests. The proxy client also allows sending multiple asynchronous
 requests, and delivering corresponding response to each request.
 [--] PartitionStateMachine: encapsulates the logic of the server
   proxy. The server logic is written without knowledge of the actual
   partitioning scheme. The library handles all communication between
   partitions and the oracle transparently.
 [--] OracleStateMachine: computes the mapping of objects to
partitions. The oracle can be configured to trigger repartitioning in different
ways: based on the number of changes to the graph, or based on some interval of
time. Our default implementation uses
METIS  to provide a
partitioning based on the workload graph. While partitioning a graph, METIS aims
to reduce the number of multi-partition commands (edge-cuts) while trying to
keep the various partitions balanced. We configured METIS to allow a 20
unbalance among partitions. METIS repartitions a graph without considering
previous partitions. Consequently, may need to relocate a large number
of objects upon a repartitioning. Other partitioning techniques could be used to
introduce incremental graph partitioning .







 

 We note one important implementation detail.  The oracle is multi-threaded: it
 can serve requests while computing a new partitioning concurrently. To ensure
 that all replicas start using the new partitioning consistently, the oracle
 identifies each partitioning with a unique id.  When an oracle replica finishes
 a repartitioning, it atomically multicasts the id of the new partitioning to
 all replicas of the oracle.  The first delivered id message defines the order
 of the new partitioning with respect to other oracle operations.





















































TPC-C benchmark




TPC-C is an industry standard for evaluating the performance of OLTP systems.
TPC-C implements a wholesale supplier company. The company has of a number of
distributed sales districts and associated warehouses. Each warehouse has 10
districts. Each district services 3,000 customers. Each warehouse maintains a
stock of 100,000 items. The TPC-C database consists of 9 tables and five
transaction types that simulate a warehouse-centric order processing
application: New-Order (45 of transactions in the workload),
Payment (43), Delivery (4), Order-Status (4) and
Stock-Level (4).

We implemented a Java version of TPC-C that runs on top of . Each row
in TPC-C tables is an object in . The oracle models the workload at the
granularity of districts, thus each district and warehouse is a node in the
graph. If a transaction accesses a district and a warehouse, the oracle will
create an edge between that district and the warehouse. The objects (e.g.,
customers, orders) that belong to a district are considered part of district.
However, if a transaction requires objects from multiple districts, only those
objects will be moved on demand, rather than the whole district. The ITEM table
is replicated in all servers, since it is not updated in the benchmark.




 social network service


Using , we have also developed a Twitter-like social network service,
named Chirper. In our social network, users can follow, unfollow, post, or read
other users' tweets according to whom the user is following. Like Twitter, users
are constrained to posting 140-character messages.

Each user in the social network corresponds to a node in a graph. If one user
follows another, a directed edge is created from the follower to the followee.
Each user has an associated timeline, which is a sequence of post
messages from the people that the user follows. Thus, when a user issues a post
command, it results in writing the message to the timeline of all the user's
followers.  In contrast, when users read their own timeline, they only need to
access the state associated with their own node.

Since DynaStare guarantees linearizable executions, any causal dependencies
between posts in  will be seen in the correct order. More
precisely, if user B posts a message after receiving a message posted by user A,
no user who follows A and B will see B's message before seeing A's message.

Overall, in , post, follow or unfollow commands can lead to
object moves.  Follow and unfollow commands can involve at most two partitions,
while posts may require object moves from many partitions.







Alternative system

We compare  to an optimized version of S-SMR 
and DS-SMR , publicly
available. 
 S-SMR scales performance with the number
of partitions under a variety of workloads. It differs from  in two
important aspects: multi-partition commands are executed by all involved
partitions, after the partitions exchange needed state for the execution of the
command, and S-SMR supports static state partitioning. In our experiments, we
manually optimize S-SMR's partitioning with knowledge about the workload. In the
experiments, we refer to this system and configuration as S-SMR*.






Performance evaluation


In this section, we report results from two benchmarks: TPC-C and
 social networking service described in the previous section.
Our experiments show that  is able to rapidly adapt to changing
workloads, while achieving throughputs and latencies far better than the existing
state-of-the-art approaches to state machine replication partitioning.


Experimental environment


We conducted all experiments on Amazon EC2 T2 large instances (nodes). Each node has 8 GB of RAM,
two virtual cores and is equipped with an Amazon EBS standard SSD with a maximal bandwidth 10000 IOPS.
All nodes ran Ubuntu Server 16.04 LTS 64 and had the OpenJDK Runtime Environment 8 with the
64-Bit Server VM (build 25.45-b02). In all experiments, the oracle
had the same resources as every other partition: 2 replicas and 3 Paxos acceptors
(in total five nodes per partition).






















TPC-C benchmark


In the experiments in this section, we deploy as many partitions as the number of warehouses.


The impact of graph repartitioning
In order to assess the impact of state partitioning on performance, we ran the TPC-C benchmark on an
un-partitioned database.  Figure  
shows the performance of with 8 warehouses and 8 partitions.
At the first part of the experiment, all the variables are randomly distributed across all partitions.
As a result, almost every transaction accesses all partitions. Thus every transaction
required coordination between partitions, and objects were constantly moving back and forth.


This can be observed in the first 50 seconds of the experiment depicted in Figure
 : low throughput (i.e., a few transactions executed per second),
high latency (i.e., up to several seconds), and a high percentage of cross-partition transactions.






After 50 seconds, the oracle computed a new partitioning based on previously executed transactions
and instructed the partitions to apply the new partitioning.
When the partitions delivered the partitioning request, they exchanged objects to achieve the new partitioning.


It takes about 10 seconds for partitions to reach the new partitioning.
During the repartitioning, transactions which access objects that are not being relocated will continue to process.
After the state is relocated, most objects involved in a transaction
can be found in a local partition, which considerably increases performance and reduces latency.









Scalability
In order to show how scales out, we varied the number of partitions from 1 to 128 partitions.


We used sufficient clients to  saturate the throughput of the system in each experiment.
Figure   shows the peak throughput of and S-SMR* as we vary the
number of partitions.
Notice that we increase the state size as we add partitions (i.e., there is one warehouse per partition).
The result shows that is capable of applying
a partitioning scheme that leads to scalable performance.








Social network


We used the Higgs Twitter dataset  as the social graph in the experiments.
The graph is a subset of the Twitter network that was built based on the monitoring of the spreading
of news on Twitter after the discovery of a new particle with the features of the elusive
Higgs boson on 4th July 2012. The dataset consists of 456631 nodes and more than 14 million edges.


We evaluate the performance of and other techniques. With S-SMR*, we used
METIS to partition the data in advance. Thus, S-SMR* started with an optimized partitioning. started with
random location of the objects. Each client issues a sequence of commands.
For each command, the client selects a random node as the active user with Zipfian access pattern ( = 0.95).
We focused on two types of workloads: timeline only commands and mix commands (85 timeline and 15 post).
Each client operates in a closed loop, that is, the client issues a command and then waits from the response to submit the next command.
































*
































vs. other techniques


Figure  shows the peak throughput and latency for approximately 75 of peak throughput
 (average and 95-th percentile) of the evaluated techniques as we vary the number of
 partitions of the fixed graph for the social networks.

In the experiment with timeline commands, all three techniques perform similarly. This happens because no moves occur in
or , and no synchronization among partitions is necessary for S-SMR* in this case.
Consequently, all three schemes scale remarkably well, and
the difference in throughput between each technique is due to the implementation of each one.














In the experiment with the mix workload, we see that  performance
decreases significantly. This happens because objects in  will only
converge if there is a perfect way to partition the data, that is, data items
can be grouped such that eventually no command accesses objects across
partitions. In the mix workload experiments, objects in  are constantly
moving back and forth between partition without converging to a stable
configuration.

In contrast, for and S-SMR*, the throughput scales with the number of
partitions in experiments with up to 8 partitions. Increasing the number of
partitions to 16 with the fixed graph reveals a tradeoff: On the one hand,
additional partitions should improve performance as there are more resources to
execute commands. On the other hand, the number of edge cuts increases with the
number of partitions, which hurts performance as there are additional operations
involving multiple partitions.

Notice that only post operations are subject to this tradeoff since they may
involve multiple partitions. The most common operation in social networks is the
request to read a user timeline. This is a single-partition command in our
application and as a consequence it scales linearly with the number of
partitions.
























Performance under dynamic workloads

Figure  depicts the performance of and S-SMR* with an evolving social network.
We started the system with the original network from Higg dataset. After 200 seconds, we introduced a new celebrity user in the workload.
The celebrity user posted more frequently, and
other users started following the celebrity.















*

At the beginning of the experiment, performance was not as good as
S-SMR* (i.e., lower throughput, higher number of percentage of multi-partition
commands, and higher number of exchanged objects), because S-SMR* started with
an optimized partitioning, while started with a random partitioning.
After 50 seconds, triggered the repartitioning process, which led to
an optimized location of data. Repartitioning helped reduce the percentage of
multi-partition commands to 10, and thus increased the throughput. After the
repartitioning, outperforms S-SMR* with the optimized partitioning.
After 200 seconds, the network started to change its structure, as many users
started to follow a celebrity, and created more edges between nodes in the
graph. Both and S-SMR suffered from the change, as the rate of
multi-partition command increased, and the throughput decreased. However, when
the repartitioning takes place in , around 300 seconds into the
execution, the previously user mapping got a better location from the oracle,
which adapted the changes. After the repartitioning, the objects are moved to a
better partition, with a resulting increase in throughput.

Figure  shows the cumulative distribution functions (CDFs) of
latency for the mix workload of and S-SMR* on different
configurations. The results suggest that S-SMR* achieves lower latency than
for 80 of the load. This is expected, as for multi-partition
commands, partitions in have to send additional data to return objects
to their original location after command execution .

























*






















The performance of the oracle

uses an oracle that maintains a global view of the workload graph. The oracle allows
to make better choices about data movement, resulting in overall
better throughput and lower latency. However, introducing a centralized
component in a distributed system is always a cause for some skepticism,
in case the component becomes a bottleneck, or a single point of failure.
To cope with failures, the oracle is implemented as a replicated partition.

The oracle keeps a mapping of objects to partitions and the relations between objects.
The size of the mapping depends on the complexity and the granularity of the graph.
In the social network dataset, where each user is modeled as an object in the workload graph,
the graph uses 1.5 GB of the oracle's memory.
In the TPC-C experiments, only district and warehouse objects are in the workload graph;
thus, the oracle only needs 1 MB of memory to store the graph for each warehouse.

The results in Figure  suggest that the oracle would not become a bottleneck.
The number of queries processed to the oracle is zero at the
beginning of the experiment, as the clients have cached the location of all objects.
After 80 seconds, the repartitioning was triggered, making all cached data on clients invalid.
Thus the throughput of queries at the oracle increases, when clients started asking for new location of variables.
However, the load diminishes rapidly and gradually reduce. This is because access to the oracle is necessary only
when clients have an invalid cache or when a repartition happens







































Conclusion


In this chapter we present , a partitioning strategy for scalable state
machine replication. is inspired by DS-SMR, a decentralized dynamic
scheme of . Differently from DS-SMR, however, performs well in
all workloads evaluated. When the state can be perfectly partitioned, 
converges more quickly than DS-SMR; when partitioning cannot avoid
cross-partition commands, it largely outperforms DS-SMR. The key insights of
are to build a workload graph on-the-fly and use an optimized
partitioning of the workload graph, computed with an online graph partitioner,
to decide how to efficiently move state variables. The chapter describes how one
can turn this conceptually simple idea into a system that sports performance
close to an optimized (but impractical) scalable system.

å



[Related works]Related works


State machine replication 
 provides strong consistency guarantees, which come from
total order and deterministic execution of commands.


Since consistent ordering is fundamental for SMR, some authors proposed to
optimize the ordering and propagation of commands.

For instance, Kapritsos and Junqueira  propose to
divide the ordering of commands between different clusters: each cluster orders
only some requests, and then forwards the partial order to every server replica,
which then merges the partial orders deterministically into a single total order
that is consistent across the system. In S-Paxos ,
Paxos  is used to order commands, but it is implemented in
a way that avoids overloading the leader process, which would turn it into a
bottleneck.

Multi-threaded execution is a potential source of non-determinism, depending on
how threads are scheduled to be executed in the operating system. Some works
attempted to circumvent this problems and come up with a multi-threaded, yet
deterministic implementation of SMR. In , Santos and
Schiper propose to parallelize the receipt and dispatching of commands, while
executing commands sequentially. In CBASE , application
semantics is used to determine which commands can be executed concurrently and
still produce a deterministic outcome (e.g., read-only commands). In
Eve , commands are tentatively executed in parallel.
After the parallel execution, replicas verify whether they reached a consistent
state; if not, commands are rolled back and re-executed sequentially.

Many database replication schemes aim at achieving high throughput by relaxing
consistency, that is, they do not ensure linearizability. In deferred-update
replication ,
replicas commit read-only transactions immediately, not always synchronizing
with each other. Although this indeed improves performance, it allows
non-linearizable executions. Database systems usually ensure serializability
 or snapshot isolation , which do not take into
account real-time precedence of different commands among different clients. For
some applications, these consistency levels may be enough, allowing the system
to scale better, but services that require linearizability cannot be implemented
with such techniques.

Efforts to make linearizable systems scalable have been made in the
past 
. In Scatter , the authors propose a
scalable key-value store based on DHTs, ensuring linearizability, but only for
requests that access the same key. In the work of Marandi et
al , variant of SMR is proposed in which data items are
partitioned but commands have to be totally ordered.
Spanner  uses a separate Paxos group per partition and,
to ensure strong consistency across partitions, clocks are assumed to be
synchronized. Although the authors say that Spanner works well with GPS and
atomic clocks, if clocks become out of synch beyond tolerated bounds,
correctness is not guaranteed.   proposes a scheme where
leases are used instead of partitions owning objects, but assumes full state
replication.   ensures consistency across
partitions without any assumption about clock synchronization, but relies on a
static partitioning of the state.   extends  by
allowing state variables to migrate across partitions in order to reduce
multi-partition commands. However,  implements repartitioning in a very
simple way that does not perform very well in scenarios where the state cannot
be perfectly partitioned.  improves on  by employing well-known
graph partitioning techniques to decide where each variable should be. Moreover,
 dillutes the cost of repartitioning by moving variables on-demand,
that is, only when they are accessed by some command.

Graph partitioning is an interesting problem with many proposed
solutions .
In this work, we do not introduce a new graph partitioning solution, but instead
we use a well-known one (METIS ) to partition the state
of a service implemented with state machine replication. Similarly to
, Schism  and Clay  also
use graph-based partitioning to decide where to place data items in a
transactional database. In either case, not much detail is given about how to
handle repartitioning dynamically without violating consistency. Turcu et al.
  proposed a technique that reduces the amount of cross-partition
commands and implements an advanced transaction routing.
Sword  is another graph-based dynamic repartitioning
technique. It uses a hypergraph partitioning algorithm to distribute rows of
tables in a relational database across database shards. Sword does not ensure
linearizability and it is not clear how it implements repartitions without
violating consistency. E-Store  is yet another repartitioning
proposal for transactional databases. It repartitions data according to access
patterns from the workload. It strives to minimize the number of multi-partition
accesses and is able to redistribute data items among partitions during
execution. E-Store assumes that all non-replicated tables form a tree-schema
based on foreign key relationships. This has the drawback of ruling out
graph-structured schemas and - relationships.  is a more
general approach that works with any kind of relationship between data items,
while also ensuring linearizability.

Some replication schemes are ``dynamic'' in that they allow the membership to be
reconfigured during execution (e.g.,
). For instance, a multicast
layer based on Paxos can be reconfigured by adding or removing acceptors. These
systems are dynamic in a way that is orthogonal to what  proposes.









[Conclusion]Conclusion


We have proposed here , a novel state-machine replication approach that
allows state-machine replication to scale by dynamically adapting to the
workload, while still ensuring strong consistency. This is a work in progress,
though, and thus needs to be further elaborated. For this purpose, a few points
need to be addressed:



    [i)]Decentralized partitioning:
    Even though defining optimized partitioning by using a centralized oracle
    shows its advantages over the decentralized approach, the oracle is still
    prone to becoming a bottleneck as the workload becomes bigger or less
    clustered, since it has to handle more queries from clients. A decentralized
    graph partitioning approach could help solve this problem.

    [ii)]Reconfiguring partitions:
    In , we considered a fixed number of partitions. Changing the
    number of partitions in a  deployment while the system is running,
    is not a trivial problem to solve efficiently. Ideally  could
    allow application's state to shrink or expand to fit into a partitioning
    configuration with more or less partitions, in order to add or remove a
    partition dynamically.



Furthermore, currently requires clients to explicitly send specific
commands to create workload graph (e.g., creating vertices or edges between
vertices). We plan to create a development environment, in the form of a
programming library, that allows the application designer to focus on the
application logic, rather than on graph partitioning and remote execution
details. The creation of the graph, handling of state partitioning and
distributed execution of commands will be handled internally by the library.







dcu




	  


