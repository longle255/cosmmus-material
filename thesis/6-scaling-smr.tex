\chapter[Scaling partitioned replicated system]{Scaling partitioned replicated system}

Replication is an area of interest that has been studied for more than three
decades in traditional domains, such as database systems, file systems, and
later in distributed object systems. Although replication is often used to
provide a fault-tolerant system, replication decisions can also help improve
performance. For instance, a service that serves mainly read data can distribute
the requests to the different replicated machines, helps to process requests in
parallel, thus, improves the throughput of the system. However, replication also
introduces daunting challenges. How such systems handle concurrent write requires
is not trivial. Therefore, the main challenge of a replicated system is to
provide the illusion of operating a single high-performance and fault-tolerant
system. In this chapter, we will survey several approaches to scaling the
performance of a replicated system.

\section{Replication and partitioning}

Replication refers to the technique of managing redundancy data on a set of
servers (replicas) in a way to ensure that each replica of the service keeps a
consistent state, given a set of consistency criteria. Replication of components
or services is used in distributed systems to achieve higher availability and
performance. Availability is the capability of the system continues to work,
even in the presence of failures, as long as the number of failures is below a
given tolerable limit. Performance refers to the response time and throughput of
the system.

Although there are many replication techniques \cite{Replication:book}, there
remain two major classes of replication techniques that have become widely
well-known to ensure this consistency: \emph{active replication} (state machine
replication) and \emph{passive replication} (primary-backup).
%Both replication techniques are useful since they have complementary qualities.
In the \emph{passive replication} approach, the requests are sent to only one
member of the replica group (the primary), which will execute the request and
send the response to the client. Any modification to the primary's state is
updated to other members of the group (the backups). If the primary fails, one
of the backups takes over the service by becoming a new primary (Figure
\ref{fig:replication:passive}). On the other hand, in the \emph{active
replication} approach, the requests of the client are sent to all members of a
given group (replicas) in the same order. The replicas will then execute the
requests as though they were the only member of the group, to reach the same
state, and reply to the client. If a client sends a request to a group of $n$
replicas (assuming no failures), then the client will receive n identical reply
to the original request (Figure \ref{fig:replication:active}). State machine
replication is a fundamentally powerful building block of this approach that
enables the implementation of highly available distributed services by
replication. State machine replication achieves strong consistency (i.e.,
linearizability) by regulating how client commands are propagated to and
executed by the replicas: every non-faulty replica must receive and execute
every command deterministically and in the same order. There are also other
replication schemes, such as chain replication \cite {chainreplication,
chainreplication:byzantine} and multi-primary replication, which is commonly
used for transactional databases that implement deferred-update replication
\cite{sciascia2012sdur, Replication:book, chundi96dur}.

\begin{figure*}[ht!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=1\columnwidth]{figures/replication-passive}
    \caption{Passive replication}
    \label{fig:replication:passive}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=1\columnwidth]{figures/replication-active}
    \caption{Active replication}
    \label{fig:replication:active}
  \end{subfigure}
  \caption{Replication techniques}
  \label{fig:replication}
\end{figure*}

Conceptually, replication can be categorized into two different categories:
full- and partial replication. Full replication means that the whole state of
the service is available on all replicas, while in partial replication, each
replica only contains a subset of the state. For example, in a distributed
database, full replication allows all rows of a table are available on all
replicated nodes, while partial replication means some replicas contain only a
subset of the rows. Intuitively, full replication often comes at a higher cost. If
all replicas have to keep the whole same state, execute the same sequence of
commands, the system can not scale. Increasing the number of replicas results in
bounded improvements in performance. On the other hand, a partially replicated
replica only stores a subset of the state and handles requests that involve that
data, thus provide scalable performance.

In other words, scalable performance can be achieved with state partitioning.
Partitioning (also known as \emph{sharding}) is a technique that divides the
state of a service in multiple partitions so that most commands access one
partition only and are equally distributed among partitions. Partitioning
replicated state machine can provide highly scalable and available systems;
however, introduce other challenges. Most services cannot be ``perfectly
partitioned''; that is, the service state cannot be divided in a way that
commands access one partition only. Therefore, a partitioning protocol must cope
with multi-partition commands.

\begin{figure*}
  \begin{minipage}[b]{1.0\linewidth}
  \centering
        \includegraphics[width=1\linewidth]{figures/replication-coordination}
  \end{minipage}
  \caption{Coordination in a partitioned replicated system}
  \label{fig:replication:coordination}
\end{figure*}

In general, we can model abstraction for partitioned replication protocols by
extending the five-phases functional model in \cite{pedone:replication}, to
cover the coordination between components in a partitioned replicated system
(see Figure \ref{fig:replication:coordination}). Those phases are:
\begin{itemize}
  \item \textit{Request phase(RE):} In the request phase, a client submits the
     to the system. This can be done in different ways: the request could
  be sent to all involved servers of the request or to one server/partition,
  then will be forwarded to the other processes. In any case, the client should
  be able to identify the involved partitions that contain the data accessed by
  the request. The client can send the request directly or use a proxy layer to
  handle the transmission.
  \item \textit{Server coordination phase (SC):} The replicas of all partitions
  that contain the data required by the request will coordinate with each other
  to synchronize the execution of the request. Depend on the consistency level
  of the application, the concurrent requests will be ordered within and/or across
  involved partitions.
  \item \textit{Execution phase (EX):} The replica servers execute the request.
  \item \textit{Agreement coordination phase (AC):} The replica servers of all involved
  partitions agree on the outcome of the execution.
  \item \textit{Response phase (END):} The result of the request is sent back to the client.
\end{itemize}

The differences between different partitioned replicated systems come from the
different approaches uses in each phase. In some cases, a phase could be omitted
(e.g., when messages are ordered by an atomic multicast/broadcast primitive in
the ordering phase (\emph{SC}), it is not necessary to run the agreement
coordination (\emph{AC}) phase). In the next few sections, we briefly describe the
performance scaling by partitioning of different replication models.
%At the end of the chapter, we compare them based on their scalability
%and consistency level provided.


\section{Scaling in distributed database systems}

In database systems, scaling is often achieved by partitioning (usually referred
as \emph{sharding}). Sharding is a way of dividing the dataset into horizontal
fragmentation, known as partitions. Each partition essentially has the same
schema and columns but contains different subsets of the total data set. The
partitions are then distributed across separate database servers, referred to as
physical shards, which can hold multiple logical partitions. Those physical
shards can be replicated to tolerate a certain degree of failure. Despite
this, the data held within all the partitions collectively represent an entire
logical dataset. To guarantee the consistency, a consensus protocol (e.g., Paxos
or Raft) is used to enforce consistency across multiple replicas of the data.
Essentially, this protocol works as a majority voting mechanism. Any change to
the dataset requires a majority of replicas to agree to the change. Google
Spanner~\cite{corbett2013spanner} and Calvin \cite{calvin} are two of the
database solutions in this category. Both systems were designed to be a
highly-scalable distributed relational database. The main difference between the
two systems is that Calvin uses a single, global consensus protocol per
database. Every transaction in Calvin participates in the same global protocol,
while Spanner applies a separate consensus protocol per partition.


\subsection{Google Spanner}

Spanner is a NewSQL \cite{Grolinger:2013tp} globally distributed database system
developed by Google \cite{corbett2013spanner}. Spanner provides features such as
global transactions, strongly consistent reads, and automatic multi-site
replication and automatic failover. Spanner partitions into multiple shards, and
replicates every shard via Paxos across independent regions for high
availability. So every operation in a transaction is a replicated operation
within a Paxos replicated state machine.

Spanner consists of multiple \emph{zones}, each of which is a deployment of
Bigtable servers. A zone uses one \emph{zonemaster} to assign data to one
hundred to several thousand sets of partitions. Each partition is a set of
Paxos-based state machines (so-called \emph{spanservers}).  To implement
transactions, including transactions span across multiple partitions, Spanner
uses two-phase locking for concurrency control, and two-phase commit. Each
spanserver implements a lock table to support two-phase-locking and a
transaction manager to support distributed transactions. Operations that
require synchronization have to acquire the lock from the lock table. The Paxos
leader in each partition is responsible for participating in these protocols on
behalf of that partition. Clients in Spanner use per-zone \emph{location
proxies} to locate the spanservers assigned to serve their data. To order the
transaction between partitions, Spanner uses TrueTime API, a combination of GPS
and atomic clocks in all of their regions (i.e., zones) to synchronize time to
within a known uncertainty bound. If two transactions are processed during time
periods that do not have overlapping uncertainty bounds, Spanner can be certain
that the later transaction will see all the writes of the earlier transaction.

\begin{figure*}
  \begin{minipage}[b]{1.0\linewidth}
  \centering
        \includegraphics[width=1\linewidth]{figures/spanner-single-partition}
  \end{minipage}
  \caption{Coordination in handling of single-partition transaction in Spanner}
  \label{fig:spanner-single-partition}
\end{figure*}

\paragraph{Single partition transaction} In Spanner, if a transaction accesses
data in a single partition (single-shard transaction), Spanner processes that
transaction as following (depicted in
Figure~\ref{fig:spanner-single-partition}). First, the client process queries
the location proxy to determine which partitions store the data accessed by the
transaction and send the transaction to the Paxos leader of that partition. The
leader acquires the locks on the involved objects and acknowledges the client.
The client executes the transaction, then initiates the commit on the leader. The
leader then coordinates with other replicas in its Paxos group to commit the
transaction, and responses to the client.

\begin{figure*}
  \begin{minipage}[b]{1.0\linewidth}
  \centering
        \includegraphics[width=1\linewidth]{figures/spanner-multi-partition}
  \end{minipage}
  \caption{Coordination in handling of multi-partition transaction in Spanner}
  \label{fig:spanner-multi-partition}
\end{figure*}

\paragraph{Multi-partition transaction} In the case the transaction accesses
more than a single partition, Paxos leaders of all involved partitions have to
coordinate and perform a two-phase commit to ensure consistency and use
two-phase locking to ensure isolation (described in
Figure~\ref{fig:spanner-multi-partition}). First, the client also needs to determine
the involved partitions by querying a location proxy. Then the client sends the
transaction to the leader of each involved group, which acquires read locks and
returns the most recent data. The client then performs the execution of the
transaction locally. To commit the transaction, the client chooses a coordinator
from the set of involved leaders, then initiate commit by sending commit message
to each leader, together with the information of the coordinator. On receiving
commit message from the client, the involved leaders coordinate to acquire write
locks by performing two-phase locking followed by two-phase commit. After having
the transaction committed on all involved partitions, the coordinator leader
sends a response to the client.

In general, the transaction processing of Spanner could be mapped to the
coordination protocol in Figure~\ref{fig:replication:coordination} as the
following:
\begin{enumerate}
  \item Client sends the transaction to the Paxos leader process.
  \item There is no initial coordination required between involved replicas.
  \item The leader(s) acquires locks and retrieves data for client. The client
  executes the transaction and initiate commit protocol.
  \item The leader(s) coordinate with others to commit the transaction.
  \item The leader sends response to the client.
\end{enumerate}

Although not detailed in the paper, Spanner allows data to be re-sharded across
\emph{spanservers} or \emph{zones} data centers to balance loads and in response
to failures by \emph{placement driver}. Periodically, \emph{placement driver}
communicates with spanservers to re-arrange data. During these transfers,
clients can still execute transactions (including updates) on the database.

\subsection{Calvin}

Calvin is a distributed transaction protocol that consists of a transaction
scheduling and replication management layer for distributed storage systems
\cite{calvin}. Similar to Spanner, Calvin also shards its data on multiple
partitions for performance, and replicate those partitions for availability. The
main difference between the two systems is the way Calvin deals with the problem
of transaction synchronization. Spanner solves that problem by a traditional
way: two-phase locking and two-phase commit, and reduce cost of synchronization
by reducing the total amount of time during which transactions hold locks using
TrueTime API.

By executing transactions using two-phase locking and two-phase commit, the
traditional database systems have no deterministic transaction order. It means
that the servers have to coordinate to preserve the order before committing the
transaction. Calvin chooses to use deterministic transaction order. This
approach could remove the overhead of coordinating after the execution phase,
since all the nodes execute the same sequence of input in a deterministic way,
to produce the same output. Calvin achieves the deterministic order by using
a sequencer (\emph{preprocessing}) to let replicas agree on the execution order and
transaction boundaries.

Client processes in Calvin first submit transactions into a distributed,
replicated log before being sent to partitions to process. The sequencer then
processes this request log, determines the order in which transactions are
executed, and establishes a global transaction input sequence. Then each replica
simply reads and processes transactions from this global order (Figure
\ref{fig:calvin}).

\begin{figure*}
  \begin{minipage}[b]{1.0\linewidth}
  \centering
        \includegraphics[width=1\linewidth]{figures/calvin}
  \end{minipage}
  \caption{Transaction processing in Calvin}
  \label{fig:calvin}
\end{figure*}

Essentially, Calvin protocol could be described as the following steps, according to
our abstraction of coordination protocol in
Figure~\ref{fig:replication:coordination}:

\begin{enumerate}
  \item Client submits the transaction to the sequencer nodes.
  \item Sequencers coordinate to order and schedule the execution of the transaction in a global sequence.,
  \item The involved replicas execute the same sequence of transactions, produce the same output.
  \item As all the replicas reach the same state, No agreement coordination is necessary.
  \item The replicas send response to the client.
\end{enumerate}

% \section{Execution models}

% In Spanner, clients can build applications by issuing multiple read and
% write requests to servers and encapsulating multiple requests into a
% transaction, which can also contain client-side arbitrary computation on the
% data. When the client commits the transaction, if the transaction spans across
% multiple partitions, one of the involved partitions plays the role of the
% coordinator to perform two-phase commit with the other groups for committing the
% transaction. Thus, transactions in Spanner’s execute-coordinate model may abort.

% In state machine replication, the application logic runs entirely at the
% replicas; clients simply invoke the operations (similar to database stored
% procedures). State machine replication relies on an atomic multicast protocol to
% totally order client requests within and across involved partitions. Each
% replica simply reads and processes requests as in the order it delivers them.
% This model makes sure the executions of requests will never abort.


\section{Scaling state machine replication with S-SMR}

% Broadly speaking, there are two classes of techniques for scaling state machine
% replication by partitioning: \emph{static} and \emph{dynamic}.
% Figure~\ref{fig:motivation} shows the result of a motivating experiment that
% compares two representative systems, one of each class. In the experiment, we
% measured the throughput and number of state moves over time with two different
% workloads: one with strong locality and one with weak locality. The workloads
% are inspired by the social network Twitter, in which the network is modeled as a
% graph, and users can ``post'' messages. The social graph was generated using a
% Zipfian distribution, where the Zipf parameter was adjusted to alter the
% locality.  For brevity, we postpone the details of the experimental setup until
% Section~\ref{sec:dynastar-experiments}.
% \emph{Static} techniques choose an immutable assignment of application state
% variables to partitions prior to executing commands. This techniques requires a
% good understanding about the workload to avoids load imbalances and favors
% single-partition commands. Moreover, many online applications experience
% variations in demand. These happen for a number of reasons. In social networks,
% for example, some users may experience a surge increase in their number of
% followers (e.g., new ``celebrities''); workload demand may shift along the hours
% of the day and the days of the week; and unexpected (e.g., a video that goes
% viral) or planned events (e.g., a new company starts trading in the stock
% exchange) may lead to exceptional periods when requests increase significantly
% higher than in normal periods. These challenges perhaps explain why most
% approaches that extend SMR with state partitioning delegate the task of
% partitioning the service state to the application designer. \emph{Dynamic}
% techniques address the limitations of static techniques by adapting the
% partitioning scheme as workloads change. For example, a dynamic technique can
% move data ``on demand'' to maximize the number of single partition user
% commands, while avoiding imbalances in the load of the partitions. The major
% challenge in designing a dynamic scheme is determining how the system selects
% the partition to which to move data.


% \begin{figure*}[ht!]
%   \centering
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.95\columnwidth]{figures/experiments/dynastar/socc-tp-strong-locality}
%     \caption{Throughput with strong locality}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.95\columnwidth]{figures/experiments/dynastar/socc-tp-weak-locality}
%     \caption{Throughput with weak locality}
%   \end{subfigure} \\
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.95\columnwidth]{figures/experiments/dynastar/socc-moves-strong-locality}
%   \caption{Number of move commands with strong locality}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.95\columnwidth]{figures/experiments/dynastar/socc-moves-weak-locality}
%     \caption{Number of move commands with weak locality}
%   \end{subfigure}
%   \caption{\dynastar, S-SMR* (i.e., optimized S-SMR) and DS-SMR under strong and weak locality, 4 partitions.}
%   \label{fig:motivation}
% \end{figure*}

% \subsection{Static state partitioning with S-SMR}
\label{sec:ssmr}

In this section, we describe Scalable state machine replication (\ssmr), an
extension to SMR that under certain workloads allows performance to grow
proportionally to the number of replicas. \ssmr\ partitions the service state
and replicates each partition. It relies on an atomic multicast primitive to
consistently order commands within and across partitions. In addition, \ssmr\
assumes a static workload partitioning. Any state reorganization requires system
shutdown and manual intervention.

In S-SMR~\cite{bezerra2014ssmr}, the service state \vvt\ is composed of $k$
partitions, in set $\Psi = \{\mathcal{V}_1, ..., \mathcal{V}_k\}$. Server group
$\ssm_i$ is assigned to partition $\mathcal{V}_i$. For brevity, we say that
server $s$ belongs to $\mathcal{V}_i$ meaning that $s \in \ssm_i$, and say
``multicast to $\mathcal{V}_i$'' meaning ``multicast to server group $\ssm_i$''.
S-SMR relies on an \emph{oracle}, which tells which partitions are accessed by
each command.

% \footnote{The oracle returns a set with the partitions accessed
% by the command, but this set does not need to be minimal; it may contain all
% partitions in the worst case, when the partitions accessed by the command cannot
% be determined before the command is executed.


\begin{figure*}
  \begin{minipage}[b]{1.0\linewidth}
  \centering
        \includegraphics[width=1\linewidth]{figures/ssmr}
  \end{minipage}
  \caption{Atomic multicast and S-SMR. (To simplify the figure, we show a single replica per partition.)}
  \label{fig:ssmr}
\end{figure*}

To execute a command, the client multicasts the command to the appropriate
partitions, as determined by the oracle. Commands that access a single partition
are executed as in classical SMR: replicas of the concerned partition agree on
the execution order and each replica executes the command independently. In the
case of a multi-partition command, replicas of the involved partitions deliver
the command and then may need to exchange state in order to execute the command
since some partitions may not have all the values read in the command. This
mechanism allows commands to execute seamlessly despite the partitioned state.

S-SMR improves on classical SMR by allowing replicated systems to scale, while
ensuring linearizability. Under workloads with multi-partition commands,
however, it has limited performance, in terms of latency and throughput
scalability. Such decreased performance when executing multi-partition commands
is due to partitions (i) exchanging state variables and (ii) synchronizing by
exchanging signals. Thus, the performance of \ssmr\ is particularly
sensitive to the way the service state is partitioned.

One way to reduce the number of multi-partition commands is by dynamically
changing the partitioning, putting variables that are usually accessed together
in the same partition. However, the partitioning oracle of \ssmr\ relies on a
static mapping of variables to partitions. One advantage of this implementation
is that all clients and servers can have their own local oracle, which always
returns a correct set of partitions for every query. Such a static mapping has
the major limitation of not allowing the service to dynamically adapt to
different access patterns.

In summary, the following steps are involved in the processing of a request in
\ssmr\, according to our functional model.

\begin{enumerate}
  \item Client sends the requests to the servers of all involved partitions.
  \item Server coordination is given by the total order property of the Atomic
  multicast.
  \item All involved replicas execute the requests in the order they are delivered.
  \item Partitions exchanges signals to ensure linearizability.
  \item All replica send back their result to the client, and the client
  typically only waits for the first answer.
\end{enumerate}

\begin{figure*}
  \begin{minipage}[b]{1.0\linewidth}
  \centering
        \includegraphics[width=1\linewidth]{figures/coordination-ssmr}
  \end{minipage}
  \caption{Atomic multicast and S-SMR. (To simplify the figure, we show a single replica per partition.)}
  \label{fig:coordination-ssmr}
\end{figure*}

% \subsection{Dynamic state partitioning}

% Dynamic techniques address the limitations of static techniques by adapting the
% partitioning scheme as workloads change

% There are two general approaches to handle multi-partition commands in terms of
% consistency guarantees. One approach, known as \emph{weak consistency}, is to
% weaken the guarantees of commands that involve multiple partitions (e.g.,
% \cite{facebookTAO}). In the context of SMR, this would mean that requests access
% data within single partition (single-partition commands) are strongly consistent
% (i.e., linearizable), while concurrent requests accessing multiple partitions
% (multi-partition commands) may lead to inconsistencies. This approach relaxes
% the consistency guarantees and makes systems less exposed to impossibility
% results \cite{FLP85, diskpaxos}, but makes the effects of data partitioning and
% replication visible to the application. To lower the chance of potential
% conflicts, data access patterns can be considered when partitioning data (i.e.,
% objects often accessed together can be co-located in the same partition). But
% these optimizations require prior knowledge about the workload, and are often
% performed offline \cite{facebookTAO}

% The other approach, known as \emph{strong consistency}, is to provide strong
% consistency guarantees for both single- and multi-partition commands and hide
% the complexity of data partitioning and replication from the application. The
% algorithms used to implement strong consistency comes with the cost of a more
% complex execution path for commands that involve multiple partitions. Some
% proposals in this category totally order requests before their execution, as in
% state machine replication, or execute requests first and then totally order the
% validation of their execution, as in distributed database systems with
% two-phase-commit.

% \subsection{Partitioning application state}

% Modern distributed systems typically scale performance by partitioning
% application state and tolerate failures by replicating each partition. Clients
% submit commands for execution to one or more partitions. Within a partition,
% replicas coordinate by means of a consensus protocol (e.g., Paxos~\cite{Lam98}).
% To coordinate the execution of multi-partition commands, replicated partitions
% rely on some distributed coordination protocol (e.g., two-phase locking
% \cite{corbett2013spanner}, optimistic concurrency control \cite{Chang:2008},
% atomic multicast \cite{bezerra2014ssmr}).

% In principle, increasing the number of partitions should result in increased
% system performance. However, if the execution of commands involves multiple
% partitions, then performance can actually decrease, due to overhead from
% ordering and coordinating commands across partitions to ensure strong
% consistency. Different techniques have been proposed to handle commands that
% access state in multiple partitions, but inherently multi-partition commands are
% more expensive than single-partition commands. Moreover, if data is not
% distributed carefully, then load imbalances can nullify the benefits of
% partitioning. Thus, an ideal partitioning scheme is one that would both (i)
% allow commands to be executed at a single partition only, and (ii) evenly
% distribute data so that load is balanced among partitions. We refer to workloads
% that can be partitioned in a way that satisfies these two properties as
% exhibiting \emph{strong locality}. Conversely, workloads that cannot avoid
% multi-partition commands with balanced load among partitions exhibit \emph{weak
% locality}.



% \subsection{Static state partitioning with S-SMR}


% \subsection{Dynamic state partitioning}

%\ssmr\ performs better as the number of multi-partition commands decreases.




% Static techniques choose an immutable assignment of application state variables
% to partitions prior to executing commands. Dynamic techniques address the
% limitations of static techniques by adapting the partitioning scheme as
% workloads change. For example, a dynamic technique can move data ``on demand''
% to maximize the number of single partition user commands, while avoiding
% imbalances in the load of the partitions.
% %Typically, this is implemented by moving data ``on demand'' to a single
% %partition which executes a user command.
% The major challenge in designing a dynamic scheme is determining how the system
% selects the partition to which to move data.


% Figure~\ref{fig:motivation} shows the result of a motivating experiment that
% compares two representative systems, one of each class. In the experiment, we
% measured the throughput and number of state moves over time with two different
% workloads: one with strong locality and one with weak locality. The workloads
% are inspired by the social network Twitter, in which the network is modeled as a
% graph, and users can ``post'' messages. The social graph was generated using a
% Zipfian distribution, where the Zipf parameter was adjusted to alter the
% locality.  For brevity, we postpone the details of the experimental setup until
% Section~\ref{sec:experiments}.


% Static techniques choose an immutable assignment of application state variables
% to partitions prior to executing commands. As an example of a static approach
% for use in the experiment, we modified the S-SMR system~\cite{bezerra2014ssmr}
% to use the static METIS~\cite{Abou-Rjeili:2006} partitioning algorithm.  The
% METIS algorithm is optimized to minimize edge-cuts when partitioning a static
% social graph. In the workload with strong locality, METIS partitions the social
% graph such that all posts are single partition; in the workload with weak
% locality, although most commands are single partition, a small fraction of posts
% involves multiple partitions. The throughput is less for the weak locality
% workload due to the more expensive execution of multi-partition posts.
% %since the static scheme incurs overhead when accessing multiple partitions to
% %execute commands.
% For workloads that do not change the graph structure and exhibit strong
% locality, the results for optimized S-SMR in Figure~\ref{fig:motivation} show a
% theoretical maximum throughput.  Of course, these results are not achievable in
% practice, since for real workloads, the graph would change over time (e.g., in
% social networks users join and leave the system, connections are created and
% removed).


% Dynamic techniques address the limitations of static techniques by adapting the
% partitioning scheme as workloads change. For example, a dynamic technique can
% move data ``on demand'' to maximize the number of single partition user
% commands, while avoiding imbalances in the load of the partitions.
% %Typically, this is implemented by moving data ``on demand'' to a single
% %partition which executes a user command.
% The major challenge in designing a dynamic scheme is determining how the system
% selects the partition to which to move data.
% %
% The second system in Figure~\ref{fig:motivation} evaluates DS-SMR, a dynamic
% partitioning strategy implemented by  Le et al.~\cite{le2016dssmr}. This system
% selects partitions randomly, which allows for a completely decentralized
% implementation, in which partitions make only local choices about data movement.
% We refer to this approach as \emph{decentralized dynamic}.  As
% Figure~\ref{fig:motivation} shows, the decentralized dynamic approach works well
% for data with strong locality, but is unstable for workloads with weak locality,
% since data is constantly moved from one partition to another.



\subsection{Conclusion}

In this chapter, we surveyed a number of approaches to partitioning a replicated
system. Replication and partitioning are widely used by several system, from
databases, file systems to distributed object systems. We introduced an abstract
framework that identifies five basic coordination steps in a partitioned
replicated system. This framework allows us to give a general comparison of the
discussed approaches... [tbd]

