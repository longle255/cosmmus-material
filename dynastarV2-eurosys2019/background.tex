%!TEX root =  main.tex
\section{Background}
\label{sec:background}

Distributed systems typically rely on sharding and replication to scale performance and tolerate failures.
More precisely, the service state \vvt\ is composed of $k$ partitions, $\ppm_1, ..., \ppm_k$, where each partition $\ppm_i$ is assigned to server group $\ssm_i$. 
(For brevity, we say that server $s$ belongs to $\ppm_i$ meaning that $s \in \ssm_i$.)
Commands that access a single partition are executed as in classical state machine replication~\cite{Lam78,Sch90}: 
replicas of the concerned partition use a consensus protocol to agree on the execution order of commands and each replica executes the commands independently. 
Existing protocols differ in (a) how to handle multi-partition commands and (b) whether state partitioning is static or dynamic.

There are three approaches to handling multi-partition commands.
Some protocols assume a workload that can be ``perfectly partitioned,'' that is, there is a way to shard the service state that avoids multi-partition commands \cite{hoang2016,Nogueira17}.
Since commands are essentially single-partition, if the load is balanced across partitions then performance scales with the number of partitions.

With protocols that support commands that span multiple partitions, the multi-partition command is executed by each involved partition, possibly after it is ordered across the involved partitions using a distributed protocol (e.g., atomic multicast, two-phase commit).
Some protocols assume the workload can be sharded such that multi-partition commands are executed locally by each of the involved partitions \cite{Mu2016}, that is, the execution of a multi-partition command in one partition does not need data stored in a different partition.
For example, to support the assignment command ``$x := y$", variables $x$ and $y$ must be placed in the same partition.
However, a command that increments $x$ and increments $z$ allows these variables to be placed in different partitions.

More general solutions do not impose restrictions on state partitioning, but must incur additional overhead in the execution of multi-partition commands \cite{bezerra2014ssmr, corbett2013spanner}.
More precisely, upon delivering a multi-partition command, replicas may need to exchange state, since some partitions may not have all the data needed in the command.
For example, consider again the assignment command ``$x := y$" in a system where $x$ and $y$ are stored in different partitions.
If the system does not support dynamic partitioning of state, then partitions can first create temporary copies of the variables they do not have so that both partitions execute the command \cite{bezerra2014ssmr}.
With dynamic partitioning of state, either $x$ can be moved to the partition that contains $y$ or vice-versa, and the command is executed at a single partition \cite{hoang2016}.

\dynastar supports dynamic partitioning of state.
It neither assumes that workloads can be perfectly partitioned nor imposes restrictions on state partitioning.
\dynastar implements multi-partition commands differently from previous proposals.
In \dynastar, one chosen partition first ``borrows'' the needed variables from the other partitions and then executes the command locally.

